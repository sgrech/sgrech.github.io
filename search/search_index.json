{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Env docs Index Cheatsheet Ansible For Devops Vagrant tutorial","title":"Home"},{"location":"#env-docs","text":"","title":"Env docs"},{"location":"#index","text":"Cheatsheet Ansible For Devops Vagrant tutorial","title":"Index"},{"location":"ansible_for_devops/","text":"Ansible for Devops notes git clone https://github.com/geerlingguy/ansible-for-devops.git Chapter 1 - Getting Started with Ansible Installing Ansible Mac brew install ansible Mac or Linux sudo pip install ansible upgrade ansible via pip pip install --upgrade ansible fedora-like systems run the following to see if EPEL repo is already available yum repolist | grep epel If no results you need to install with the following commands rpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/\\ epel-release-6-8.noarch.rpm yum install epel-release yum -y install ansible Debian systems sudo apt-add-repository -y ppa:ansible/ansible sudo apt-get update sudo apt-get install -y ansible if missing add-apt-repository sudo apt-get install python-software-properties Creating a basic inventory file ansible uses an inventory file to communicate with your servers create file at /etc/ansible/hosts and add one server to it sudo mkdir /etc/ansible sudo touch /etc/ansible/hosts edit hosts file and put the following in it [example] wwww.example.com since I am using vagrant for vm here is the hosts file I am using [example] ansible-devops-1 ansible_ssh_port=22 ansible_ssh_user=vagrant ansible_ssh_private_key_file=~/Workspace/tmp/ansible-devops/.vagrant/machines/default/virtualbox/private_key setting up server with vagrant Vagrant.configure(\"2\") do |config| config.vm.box = \"hashicorp/bionic64\" config.vm.hostname = \"ansible-devops-1\" config.vm.network \"private_network\", ip: \"192.168.50.101\" end Running first ad-hoc ansible command ansible example -m ping -u [username] Note: ansible assumes you are using passwordless (key-based) login for SSH You can read a primer on SSH/OpenSSH/keys here A useful command: ansible example -a \"free -m\" -u [username] Chapter 2 - Local Infrastructure Development: Ansible and Vagrant Setting up vagrant vagrant box add geerlingguy/centos7 vagrant init geerlingguy/centos7 vagrant up Using Ansible with vagrant Open Vagrant file and edit it Vagrant.configure(\"2\") do |config| config.vm.box = \"geerlingguy/centos7\" # Provisioning configuration for Ansible config.vm.provision \"ansible\" do |ansible| ansible.playbook = \"playbook.yml\" end end Create ansible playbook playbook.yml --- - hosts: all become: yes tasks: - name: Ensure NTP (for time synchronization) is installed. yum: name=ntp state=present - name: Ensure NTP is running service: name=ntpd state=started enabled=yes --- is a marker showing document is formatted in YAML - hosts: all tells ansible which hosts this playbook applies. all works here since Vagrant is invisibly usings its own Ansible inventory file become: yes since priviliged access is needed to install NTP and do system configs, this tells Ansible to use sudo for all tasks in playbook tasks: All tasks after this line will run on all hosts` the following command is equivalent of running yum install ntp but also checks if ntp is installed before - name: Ensure NTP (for time synchronization) is installed. yum: name=ntp state=present the following line checks and ensures ntpd service is started and running, and sets it to start at system boot - name: Ensure NTP is running service: name=ntpd state=started enabled=yes Note: you can leave out the name module but it is useful as it is a piece of documentation --- - hosts: all become: yes tasks: - yum: name=ntp state=present - service: name=ntpd state=started enabled=yes Cleaning up you can remove the vagrant machine from your system by running vagrant destroy if need be you can rebuild again using vagrant up Chapter 3 - Ad-Hoc Commands Build infrastructure with Vagrant for testing we are going to manage three VMs: two app servers and a database server # -*- mode: ruby -*- # vi: set ft=ruby : # VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| # General Vagrant VM configuration config.vm.box = \"geerlingguy/centos7\" config.ssh.insert_key = false config.vm.synced_folder \".\", \"/vagrant\", disabled: true config.vm.provider :virtualbox do |v| v.memory = 256 v.linked_clone = true end # Application server 1 config.vm.define \"app1\" do |app| app.vm.hostname = \"orc-app1.test\" app.vm.network :private_network, ip: \"192.168.60.4\" end # Application server 2 config.vm.define \"app2\" do |app| app.vm.hostname = \"orc-app2.test\" app.vm.network :private_network, ip: \"192.168.60.5\" end # Database server config.vm.define \"db\" do |db| db.vm.hostname = \"orc-db.test\" db.vm.network :private_network, ip: \"192.168.60.6\" end end # Application servers [app] 192.168.60.4 192.168.60.5 # Database server [db] 192.168.60.6 # Group 'multi' with all servers [multi:children] app db # Variables that will be applied to all servers [multi:vars] ansible_ssh_user=vagrant ansible_ssh_private_key_file=~/.vagrant.d/insecure_private_key the first block puts both of app servers into an app group the second block puts the database server into a db group the third block tells ansible to define a new group multi with child groups the fourth block adds variables to the multi group that will be applied to all servers within multi and all its children Your first adhoc commands check vagrant configured VMs with right hostnames use -a argument hostname to run hostname on all servers ansible multi -a \"hostname\" by default ansible will run commands in parallel, using multiple process forks you can also tell ansible to run command using only one fork ansible multi -a \"hostname\" -f 1 check disk space avaiable ansible multi -a \"df -h\" check free memory ansible multi -a \"free -m\" check date and time ansible multi -a \"date\" Make changes using Ansible modules install NTP daemon on server to keep the time in sync ansible multi -b -m yum -a \"name=ntp state=present\" Note: -b option tells ansible to run command with become (i.e. run with sudo ) Note: if running command against server requiring password use -K so you can enter password make sure NTP daemon is started and set to run on boot ansible multi -b -m service -a \"name=ntpd state=started enabled=yes\" even if running shell commands you could wrap them in Ansible's shell or command modules ansible multi -m shell -a \"date\" check to make sure our servers are synced closely to official time in NTP server ansible multi -b -a \"service ntpd stop\" ansible multi -b -a \"ntpdate =q 0.rhel.pool.ntp.org\" ansible multi -b -a \"service ntpd start\" Configure application servers ansible app -b -m yum -a \"name=MySQL-python state=present\" ansible app -b -m yum -a \"name=python-setuptools state=present\" ansible app -b -m easy_install -a \"name=django<2 state=present\" you could install django using pip , which can be install via easy_install Ansible's easy_install module doesn't allow you to uninstall packages lil pip does to make sure Django is installed and working correctly: ansible app -a \"python -c 'import django; \\ print django.get_version()'\" Configure the Database servers install mariadb, start it and configure firewall to allow access on 3306 ansible db -b -m yum -a \"name=mariadb-server state=present\" ansible db -b -m service -a \"name=mariadb state=started enabled=yes\" ansible db -b -a \"iptables -F\" ansible db -b -a \"iptables -A INPUT -s 192.168.60.0/24 -p tcp -m tcp --dport 3306 -j ACCEPT\" set up mariadb with access for one user ansible db -b -m yum -a \"name=MySQL-python state=present\" ansible db -b -m mysql_user -a \"name=django host=% password=12345 priv=*.*:ALL state=present\" Making changes to just one server check service status and restart it on one server ansible app -b -a \"service ntpd status\" ansible app -b -a \"service ntpd restart\" --limit \"192.168.60.4\" we used the --limit argument to limit the command to a specific host in the specified group --limit will match either an exact string or a regular expression prefixed with ~ # Limit hosts with a simple pattern (asterisk is a wildcard) ansible app -b -a \"service ntpd restart\" --limit \"*.4\" # Limit hosts with a regular expression (prefix with a tilde) ansible app -b -a \"service ntpd restart\" --limit ~\".*\\.4\" Try to reserve the --limit option for running on single servers, if you often find yourself running on same set of server consider adding them to a group in your inventory Manages users and groups ansible's users and group modules make user management easy and standard across linux flavors add an admin group on the app server for server admins ansible app -b -m group -a \"name=admin state=present\" you can remove a group by setting state=absent set a group id with gid=[gid] and indicate group is a system group with system=yes add user johndoe to app server with group just created and give him a home folder ansible app -b -m user -a \"name=johndoe group=admin createhome=yes\" if you want to create an SSH key for the new user you can use generate_ssh_key=yes you can also set UID of user uid=[uid] user's shell shell=[shell] and password password=[encrypted-password] if you want to delete an account: ansible app -b -m user -a \"name=johndoe state=absent remove=yes\" Manage packages ansible has a variety of package management modules for any flavor of linux there is also a generic package module that can be used for easier cross-platform ansible usage if you want to install a generic package like git on any system, you can use: ansible app -b -m package -a \"name=git state=present\" Manage files and directories Get information about a file ansible multi -m stat -a \"path=/etc/environment\" Copy a file to the servers while Ansible has more advanced file copy modules like rsync most file copy operations can be completed with Ansible's copy module ansible multi -m copy -a \"src=/etc/hosts dest=/tmp/hosts\" src can be a file or directory if you include a trailing slash, only contents of the directory will be copied into dest if you omit trailing slash, the contents and directory will be copied to dest copy module is perfect for single-file copies, and small directories if you want to copy hundreds of files, you should consider either copying an archive and expanding with unarchive module, or using synchronize or rysnc modules Retrieve a file from servers fetch module works exactly like copy module but in reverse use the following command to grab hosts file from servers ansible multi -b -m fetch -a \"src=/etc/hosts dest=/tmp\" by default fetch will put the file from each server into a folder in the destination with the name of the host thus the server hosts file will end up in /tmp/192.168.60.6/etc/hosts if filenames are unique across servers you can use flat=yes and dest=/tmp/ to fetch directly into /tmp directory Create directories and files file module used to create files and directories, manage permissions, and ownershoips, modify SELinux properties, and create symlinks here's how to create a directory: ansible multi -m file -a \"dest=/tmp/test mode=644 state=directory\" here's how to create a symlink ansible multi -m file -a \"src=/src/file dest=/dest/symlink state=link\" Delete directories and files ansible multi -m file -a \"dest=/tmp/test state=absent\" other file-management modules include lineinfile , ini_file , and unarchive Run operations in the background for long running operations (such as apt-get update ) you can tell Ansible to run commands asynchronously and poll servers to see when commands finish -B <seconds> the maximum amount of time to let job run -P <seconds> the amount of time to wait between polling the servers for updates Note: as of Ansible 2.0, async polling no longer displays output in real time, this is an open bug Update servers asynchronously run yum -y update on all servers and set -P 0 to fire command on all server and print background job information ansible multi -b -B 3600 -P 0 -a \"yum -y update\" you can check the status using async_status module as long as you have ansible_job_id value to pass ansible multi -b -m async_status -as \"jid=[ansible_job_id] Check log files common file operations like tail , cat , grep , etc... work through ansible command with a few caveats Operations that continously monitor a file won't work because Ansible only displays output after an operation is complete it's not a good idea to run a command that returns a huge amount of data if you redirect and filter output from a command you need to use shell module instead of default command module ansible multi -b -a \"tail /var/log/messages\" if you want to filter messages log with something like grep, use shell ansible multi -b -m shell -a \"tail /var/log/messages | grep ansible-command | wc -l\" Manage cron jobs you can manage cron jobs with the cron module` the following runs a shell script every day at 4 a.m ansible multi -b -m cron -a \"name='daily-cron-all-servers' hour=4 job='/path/to/daily-script.sh'\" ansible will assume <asterisk> for all values not specified valid values are day , hour , minute , month , and weekday you can also specify special time values like reboot , yearly , or monthly using `special_time=[value] you can set the user the job will run under via user=[user] you can create a backup of current crontab via backup=yes if you want to remove a corn job use state=absent ansible multi -b -m cron -a \"name='daily-cron-all-servers' state=absent\" you can also manage custom crontab files by specifing the location of the cron file with cron_file=cron_file_name Deploy version-controlled application for simple deployments, ad-hoc commands can help ansible app -b -m git -a \"repo=git://example.com/path/to/repo.git \\ dest=/opt/myapp update=yes version=1.2.4\" git module lets you specifiy branch, tag or even specific commit to force update checked-out copy we use update=yes if git is not installed you can use the following ansible app -b -m package -a \"name=git state=present\" if you get \"unkown hostkey\" add accept_hostkey=yes or add hostkey to you server's known_hosts run the application's update.sh shell script ansible app -b -a \"/opt/myapp/update.sh\" Ansible's SSH connection history ansible uses standard and secure SSH connection to communicate with servers on thing that is universal to all of ansible's SSH conection methods is that it uses the connection to transfer one or a few files defining play or command to the remote server, runs the play/command, then deletes the transferred files and reports back the results Faster OpenSSH with Pipelining modern versions of Ansible allow you to improve performance of default OpenSSH implementation instead of copying files, running them on remote server, then removing them, the pipelining method will send and execute commands of most modules directly over SSH connection this can be enabled via pipelining=true under the ssh_connection section of the ansible config file ansible.cfg Note: you need to comment out Defaults requiretty option in /etc/sudoers for this to work well, it should be commented out by default but best to double check Chapter 4 - Ansible Playbooks Power plays it is easy to convert shell scripts directly into ansible playbooks # Install apache yum install --quiet -y httpd httpd-devel # Copy configuration file cp httpd.conf /etc/httpd/conf/httpd.conf cp httpd-vhosts.conf /etc/httpd/conf/httpd-vhosts.conf # Start Apache and configure it to run at boot service httpd start chkconfig httpd on --- - hosts: all tasks: - name: Install Apache. command: yum install --quiet -y httpd httpd-devel - name: copy config file command: > cp httpd.conf /etc/httpd/conf/httpd.conf - command: > cp httpd-vhosts.conf /etc/httpd/conf/httpd-vhosts.conf - name: Start Apache and configure it to run at boot command: service httpd start - command: chkconfig httpd on to run the playbook you would call it using the following command ansible-playbook playbook.yml Revised Ansible Playbook - Now with idempotency! the above playbook will perform exactly like the shell script, but you can improve by using ansible's built-in modules --- - hosts: all become: yes tasks: - name: Install Apache yum: name: - httpd - httpd-devel state: present - name: Copy config files copy: src: \"{{ item.src }}\" dest: \"{{ item.dest }}\" owner: root group: root mode: 0644 with_items: - src: httpd.conf dest: /etc/httpd/conf/httpd.conf - src: httpd-vhosts.conf dest: /etc/httpd/conf/httpd-vhosts.conf - name: Make sure Apache is started now and at boot service: name=httpd state=started enabled=yes become: yes runs all commands through sudo we tell yum to make sure packages installed with state: present , but we could also use state: latest to ensure latest version or state: absent to make sure it packages not installed you can pass lists of variables to the tasks using with_items: and reference them using the item variable {{ item }} you can list as many variables as you want, even deeply-nested dicts this format of playbook is idempotent running the playbook with the --check option verifies the configuration matches waht's defined in the playbook without running the tasks on the server Running playbooks running the above playbooks will run it across every host defined in your ansible inventory Limiting playbooks to particular hosts and groups you can limit a playbook by changing the hosts: definition the value can be set to all hosts, a group of hosts, multiple groups of hosts (e.g. webservers,dbservers , individual hosts (e.g. atl.example.com ), or a mixture of hosts you can do wild card matches like *.example.com you can also limit hosts via the following command ansible-playbook playbook.yml --limit webservers you can also limit playbook to one particular host ansible-playbook playbook.yml --limit xyz.example.com to see list of hosts that would be affected by your playbook before you run it: ansible-playbook playbook.yml --list-hosts playbook: playbook.yml play #1 (all): all TAGS: [] pattern: ['all'] hosts (3): 192.168.60.6 192.168.60.5 192.168.60.4 Setting user and sudo options if no remote_user is defined alongside hosts in a playbook, it assumes you'll connect as the user defined in your inventory file for a particular host, then it will fall back to your local user account name you can explicitly define a remote user with the --user (-u) option ansible-playbook playbook.yml --user=johndoe when you need to pass sudo password to remote server, use --ask-become-pass (-K) option you can force all tasks in playbook to use sudo with --become (-b) option you can define the sudo user for the tasks run via sudo (the default is root) with the --become-user (-U) option ansible-playbook playbook.yml --become --become-user=janedoe --ask-become-pass if you are not using key-based auth to connect to servers you can use --ask-pass Other options --inventory=PATH (-i PATH) define a custom inventory file (default is located at /etc/ansible/hosts --verbose (-v) verbose mode, you can pass -vvvv to give every detail --extra-vars=VARS (-e VARS) define variables to be used in the playook in \"key=value,key=value\" format --forks=NUM (-f NUM) set this number higher than 5 to increase the number of servers tasks will run on concurrently --connection=TYPE (-c TYPE) the type of connection which will be used, this defaults to ssh but you can use local` to run a playbook on your local machine, or on a remote server via cron --check run playbook in Check Mode ('Dry Run') Real-World playbook: CentOS Node.js app server # install EPEL repo yum install -y epel-release # Import Remi GPG key wget https://rpms.remirepo.net/RPM-GPG-KEY-remi \\ -O /etc/pki/rpm-gpg/RPM-GPG-KEY-remi rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-remi # Install rpm -Uvh --quiet \\ https://rpms.remirepo.net/enterprise/remi-release-7.rpm # Install Node.js (npm plus all its dependencies) yum --enablerepo=epel -y install npm if you wanted to skip adding GPG keys, just run commands with --nogpgcheck in Ansible set the disable_gpg_check parameter of yum module to yes this is not a good idea, GPG stands for GNU Privacy Guard and it is a way for devs and package dists to sign their packages . \u251c\u2500\u2500 Vagrantfile \u2514\u2500\u2500 provisioning \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 app.js \u2502 \u2514\u2500\u2500 package.json \u2514\u2500\u2500 playbook.yml 2 directories, 4 files Vagrantfile # -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = \"geerlingguy/centos7\" config.vm.hostname = \"nodejs.test\" config.vm.network :private_network, ip: \"192.168.55.55\" config.ssh.insert_key = false config.vm.synced_folder \".\", \"/vagrant\", disabled: true config.vm.provider :virtualbox do |v| v.memory = 256 end # Ansible provisioner. config.vm.provision :ansible do |ansible| ansible.playbook = \"provisioning/playbook.yml\" end end provisioning/playbook.yml --- - hosts: all become: yes vars: node_apps_location: /usr/local/opt/node tasks: - name: Install EPEL repo. yum: name=epel-release state=present - name: Import Remi GPG key. rpm_key: key: \"https://rpms.remirepo.net/RPM-GPG-KEY-remi\" state: present - name: Install Remi repo. yum: name: \"https://rpms.remirepo.net/enterprise/remi-release-7.rpm\" state: present - name: Ensure firewalld is stopped (since this is a test server). service: name=firewalld state=stopped - name: Install Node.js and npm. yum: name=npm state=present enablerepo=epel - name: Install Forever (to run our Node.js app). npm: name=forever global=yes state=present - name: Ensure Node.js app folder exists. file: \"path={{ node_apps_location }} state=directory\" - name: Copy example Node.js app to server. copy: \"src=app dest={{ node_apps_location }}\" - name: Install app dependencies defined in package.json. npm: \"path={{ node_apps_location }}/app\" - name: Check list of running Node.js apps. command: forever list register: forever_list changed_when: false - name: Start example Node.js app. command: \"forever start {{ node_apps_location }}/app/app.js\" when: \"forever_list.stdout.find(node_apps_location + '/app/app.js') == -1\" provisioning/app/app.js // Simple Express web server. // @see http://howtonode.org/getting-started-with-express // Load the express module. var express = require('express'); var app = express(); // Respond to requests for / with 'Hello World'. app.get('/', function(req, res){ res.send('Hello World!'); }); // Listen on port 80 (like a true web server). app.listen(80); console.log('Express server started successfully.'); provisioning/app/package.json { \"name\": \"examplenodeapp\", \"description\": \"Example Express Node.js app.\", \"author\": \"Jeff Geerling <geerlingguy@mac.com>\", \"dependencies\": { \"express\": \"4.x\" }, \"engine\": \"node >= 0.10.6\" } notes yum install EPEL repo (and automatically imports GPG key) rpm_key is a simple module that takes and imports RPM key from URL or file, or the key id of a key that is already present, and ensures the key is either present or absent firewall is disabled for testing purposes yum installs Node.js along with all required packages for npm , allows EPEL repo to be searched via enablerepo parameter (you can explicity disable a repo via disablerepo ) use npm module to install Node.js utility forever to launch app and keep running, setting global to yes tells NPM to install forever module in /usr/lib/node_modules/ Note: quotes are being used in YAML when there are Jinja variable (e.g. {{ var }} ) or when there are colons (:) in a string (e.g. URLs) register creates a new variable, forever_list , to be used in next play to determine when to run the play, it stashes the output (stdout, stderr) of defined command in variable passed to it changed_when tells when this play results in a change to the server, in this case forever list will ever change the server, so we just say false app is started using 'forever', we could start it by calling node {{ node_apps_location }}/app/app.js but we would not be able to control process easily and would also need to use nohup and & to avoid ansible hanging to avoid running multiple instances of node app, we start it using when and tell it to start only when the app's path is not in the forever list output Real-world playbook: Ubuntu LAMP server with drupal . \u251c\u2500\u2500 Vagrantfile \u2514\u2500\u2500 provisioning \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 playbook.yml \u251c\u2500\u2500 templates \u2502 \u2514\u2500\u2500 drupal.test.conf.j2 \u2514\u2500\u2500 vars.yml 2 directories, 5 files Vagrantfile # -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = \"geerlingguy/ubuntu1604\" config.vm.network :private_network, ip: \"192.168.88.8\" config.vm.hostname = \"drupal.test\" config.ssh.insert_key = false config.vm.provider :virtualbox do |v| v.memory = 1024 end # Ansible provisioning. config.vm.provision \"ansible\" do |ansible| ansible.playbook = \"provisioning/playbook.yml\" ansible.config_file = \"provisioning/ansible.cfg\" ansible.verbose = \"vvvv\" end end provisioning/ansible.cfg [defaults] allow_world_readable_tmpfiles = true log_path = /var/log/ansible.log provisioning/playbook.yml --- - hosts: all become: yes vars_files: - vars.yml pre_tasks: - name: Update apt cache if needed. apt: update_cache=yes cache_valid_time=3600 handlers: - name: restart apache service: name=apache2 state=restarted tasks: - name: Get software for apt repository management. apt: state: present name: - python-apt - python-pycurl - name: Add ondrej repository for later versions of PHP. apt_repository: repo='ppa:ondrej/php' update_cache=yes - name: \"Install Apache, MySQL, PHP, and other dependencies.\" apt: state: present name: - git - curl - unzip - sendmail - apache2 - php7.1-common - php7.1-cli - php7.1-dev - php7.1-gd - php7.1-curl - php7.1-json - php7.1-opcache - php7.1-xml - php7.1-mbstring - php7.1-pdo - php7.1-mysql - php-apcu - libpcre3-dev - libapache2-mod-php7.1 - python-mysqldb - mysql-server - name: Disable the firewall (since this is for local dev only). service: name=ufw state=stopped - name: \"Start Apache, MySQL, and PHP.\" service: \"name={{ item }} state=started enabled=yes\" with_items: - apache2 - mysql - name: Enable Apache rewrite module (required for Drupal). apache2_module: name=rewrite state=present notify: restart apache - name: Add Apache virtualhost for Drupal 8. template: src: \"templates/drupal.test.conf.j2\" dest: \"/etc/apache2/sites-available/{{ domain }}.test.conf\" owner: root group: root mode: 0644 notify: restart apache - name: Symlink Drupal virtualhost to sites-enabled. file: src: \"/etc/apache2/sites-available/{{ domain }}.test.conf\" dest: \"/etc/apache2/sites-enabled/{{ domain }}.test.conf\" state: link notify: restart apache - name: Remove default virtualhost file. file: path: \"/etc/apache2/sites-enabled/000-default.conf\" state: absent notify: restart apache - name: Adjust OpCache memory setting. lineinfile: dest: \"/etc/php/7.1/apache2/conf.d/10-opcache.ini\" regexp: \"^opcache.memory_consumption\" line: \"opcache.memory_consumption = 96\" state: present notify: restart apache - name: Create a MySQL database for Drupal. mysql_db: \"db={{ domain }} state=present\" - name: Create a MySQL user for Drupal. mysql_user: name: \"{{ domain }}\" password: \"1234\" priv: \"{{ domain }}.*:ALL\" host: localhost state: present - name: Download Composer installer. get_url: url: https://getcomposer.org/installer dest: /tmp/composer-installer.php mode: 0755 - name: Run Composer installer. command: > php composer-installer.php chdir=/tmp creates=/usr/local/bin/composer - name: Move Composer into globally-accessible location. command: > mv /tmp/composer.phar /usr/local/bin/composer creates=/usr/local/bin/composer - name: Check out drush 8.x branch. git: repo: https://github.com/drush-ops/drush.git version: 8.x dest: /opt/drush - name: Install Drush dependencies with Composer. command: > /usr/local/bin/composer install chdir=/opt/drush creates=/opt/drush/vendor/autoload.php - name: Create drush bin symlink. file: src: /opt/drush/drush dest: /usr/local/bin/drush state: link - name: Check out Drupal Core to the Apache docroot. git: repo: https://git.drupal.org/project/drupal.git version: \"{{ drupal_core_version }}\" dest: \"{{ drupal_core_path }}\" register: git_checkout - name: Ensure Drupal codebase is owned by www-data. file: path: \"{{ drupal_core_path }}\" owner: www-data group: www-data recurse: true when: git_checkout.changed | bool - name: Install Drupal dependencies with Composer. command: > /usr/local/bin/composer install chdir={{ drupal_core_path }} creates={{ drupal_core_path }}/vendor/autoload.php become_user: www-data - name: Install Drupal. command: > drush si -y --site-name=\"{{ drupal_site_name }}\" --account-name=admin --account-pass=admin --db-url=mysql://{{ domain }}:1234@localhost/{{ domain }} --root={{ drupal_core_path }} creates={{ drupal_core_path }}/sites/default/settings.php notify: restart apache become_user: www-data provisioning/vars.yml --- # The core version you want to use (e.g. 8.8.x, 8.9.x). drupal_core_version: \"8.8.x\" # The path where Drupal will be downloaded and installed. drupal_core_path: \"/var/www/drupal-{{ drupal_core_version }}-dev\" # The resulting domain will be [domain].test (with .test appended). domain: \"drupal\" # Your Drupal site name. drupal_site_name: \"Drupal Test\" provisioning/template/drupal.test.conf.j2 <VirtualHost *:80> ServerAdmin webmaster@localhost ServerName {{ domain }}.test ServerAlias www.{{ domain }}.test DocumentRoot {{ drupal_core_path }} <Directory \"{{ drupal_core_path }}\"> Options FollowSymLinks Indexes AllowOverride All </Directory> </VirtualHost> notes you can run tasks before or after the main tasks (defined in tasks: ) or roles (defined in roles: ) using pre_tasks and post_tasks in this case we ensure apt cache is updated before rest of playbook, and we tell it to update cache if it's more than 3600 seconds (1 hour) since last update handlers are special kinds of tasks you run at the end of a play by adding notify option to any of the tasks in that group handlers will only be called if one of the tasks notifying the handler makes a change to the server (and doesn't fail), and it will be notified at the end of the play in this case, the handler has been defined to restart apache2 service after a configuration change just like variables, handlers and tasks may be placed in separate files to keep things tidy when a task fails, playbook execution is stopped and handlers aren't notified and triggered if oyou want to make sure handlers always run after a task uses notify even after failure, add --force-handlers in your ansible-playbook command python-apt and python-pycurl are helper libraries that allow python to manage apt more precisely, such as for apt_repository module linefile module ensures a particular line of text exists or not in a file command module is preferred option for running commands on a host and it works on most scenarios yet command does not run the command via remote shell /bin/sh so options like <, >, |, & and local environment vars like $HOME do not work shell allows you to pipe command output to other commands, access local environment, etc script executes shell scripts, though it is always better to use idempotent playbooks raw executes raw commands via ssh (should only be ever used as a last resort) Real-world playbook: Ubuntu server with Solr . \u251c\u2500\u2500 Vagrantfile \u2514\u2500\u2500 provisioning \u251c\u2500\u2500 playbook.yml \u2514\u2500\u2500 vars.yml 1 directory, 3 files Vagrantfile # -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = \"geerlingguy/ubuntu1604\" config.vm.network :private_network, ip: \"192.168.66.66\" config.vm.hostname = \"solr.test\" config.ssh.insert_key = false config.vm.provider :virtualbox do |v| v.memory = 1024 end # Ansible provisioner. config.vm.provision :ansible do |ansible| ansible.playbook = \"provisioning/playbook.yml\" end end provisioning/playbook.yml --- - hosts: all become: true vars_files: - vars.yml pre_tasks: - name: Update apt cache if needed. apt: update_cache=true cache_valid_time=3600 tasks: - name: Install Java. apt: name=openjdk-8-jdk state=present - name: Download Solr. get_url: url: \"https://archive.apache.org/dist/lucene/solr/{{ solr_version }}/solr-{{ solr_version }}.tgz\" dest: \"{{ download_dir }}/solr-{{ solr_version }}.tgz\" checksum: \"{{ solr_checksum }}\" - name: Expand Solr. unarchive: src: \"{{ download_dir }}/solr-{{ solr_version }}.tgz\" dest: \"{{ download_dir }}\" remote_src: true creates: \"{{ download_dir }}/solr-{{ solr_version }}/README.txt\" - name: Run Solr installation script. command: > {{ download_dir }}/solr-{{ solr_version }}/bin/install_solr_service.sh {{ download_dir }}/solr-{{ solr_version }}.tgz -i /opt -d /var/solr -u solr -s solr -p 8983 creates={{ solr_dir }}/bin/solr - name: Ensure solr is started and enabled on boot. service: name=solr state=started enabled=yes provisioning/vars.yml --- # The directory into which Solr will be downloaded for setup. download_dir: /tmp # The directory inside which Solr will be installed. solr_dir: /opt/solr # Solr version and download information. solr_version: 8.2.0 solr_checksum: sha512:beb4e37fc21bf483e3b6bae43cb06a49bc420a0f2b920c97909a69a5efeacba1e7d2ff09ae8018446c87bf007f88f06a59de73cd1923f0967e8206629b0509b6 notes when downloading files from remote servers, get_url module provides more flexibility than raw wget or curl commands use a full path to download file otherwise it will be re-downloaded on subsequent runs of the playbook you can use checksum to make sure it is the file you are expecting, if it doesn't match the file will be discarded when using unarchive we use creates option to make operation idempotent unarchive module docs show you can consolidate get_url and unarchive into one task, but since Solr installation requires original archive to be present we still need both tasks Chapter 5 - Ansible Playbook - Beyond the Basics Handlers previously we used a simple handler to restart Apache, and tasks that affected Apache configs notified the handler with the option notify: restart apache handlers: - name: restart apache service: name=apache2 state=restarted tasks: - name: Enable Apache rewrite module apache2_module: name=rewrite state=present notify: restart apache if you want to notify multiple handlers from one task, use a list for notify option - name: Rebuild application configuration command: /opt/app/rebuild.sh notify: - restart apache - restart memcached you can have handlers notify other handlers by adding a notify option handlers: - name: restart apache service: name=apache2 state=restarted notify: restart memcached - name: restart memcached service: name=memcached state=restarted handlers will only run if a task notifies the handler handlers will run only once at the end of the play if you need to override run once behaviour, use meta: flush_handlers if play fails on a host before handlers are notified, handlers will never be run if you want handlers to run even when play has failed, you can use meta module or --force-handlers flag Environmental variables there are multiple ways to work with env vars if you want to set env vars for remote user account you can add lines to user's .bash_profile - name: Add an environment variable to the remote user shell lineinfile: \"dest=~/.bash_profile regexp=^ENV_VAR= line=ENV_VAR=value\" all subsequent shell tasks will have access to this env variable, since only shell module understands shell commands that use env var to use an env var in further tasks, it's best to use a task register option to store env var in a variable Ansible can later use - name: Add an environment variable to the remote user shell lineinfile: \"dest=~/.bash_profile regexp=^ENV_VAR= line=ENV_VAR=value\" - name: Get the value of the environment variable we just added shell: 'source ~/.bash_profile && echo $ENV_VAR' register: foo - name: Print the value of the environment variable debug: msg=\"The variable is {{ foo.stdout }} we use source ~/.bash_profile to make sure we are using the latest env config for the remote user in some situations, the tasks all run over a persistent or semi-cached SSH session, over which $ENV_VAR would not yet be defined Note: there are many diff places you can store env vars, like .bashrc , .profile , and .bash_login in a user home folder. In this case we want the env var to be available to Ansible, which runs a pseudo-TTY shell session, in which case ~/.bash_profile is used to configure the environment. You can read more in Configuring your login sessions with dotfiles Linux will also read global env vars added to `/etc/environment so you can add your variable there - name: Add a global environment variable lineinfile: \"dest=/etc/environment regexp=^ENV_VAR= line=ENV_VAR=value\" become: yes if you require many env vars to be set, you might consider copy or template with a local file Pre-play environment variables you can set the environment for just one play using environment option for that play - name: Download a file using example-proxy as a proxy get_url: url=http://www.example.com/file.tar.gz dest=~/Downloads/ environment: http_proxy: http://example-proxy:80/ if you have many tasks that require a proxy, or some env var, you can pass them via your playbook vars section vars: proxy_vars: http_proxy: http://example-proxy:80/ https_proxy: https://example-proxy:443/ tasks: - name: Download a file using example-proxy as a proxy get_url: url=http://www.example.com/file.tar.gz dest=~/Downloads/ environment: proxy_vars if a proxy needs to be set system-wide you can do so using global /etc/environment file # In the 'vars' section of the playbook (set to 'absent' to disable proxy) proxy_state: present # in the 'tasks' section of the playbook - name: Configure the proxy lineinfile: dest: /etc/environment regexp: \"{{ item.regexp }}\" line: \"{{ item.line }}\" state: \"{{ proxy_state }}\" with_items: - regexp: \"^http_proxy=\" line: \"http_proxy=http://example-proxy:80/\" - regexp: \"^https_proxy=\" line: \"https_proxy=https://example-proxy:443/\" - regexp: \"^ftp_proxy=\" line: \"ftp_proxy=http://example-proxy:80/\" doing it this way allows us to configure whether the proxy is enabled per-server (using the proxy_state var) with one play Note: you can test remote env vars using the ansible command ansible test -m shell -a 'echo $TEST' . Be careful when doing so as using the wrong quotes you might end up printing a local env var instead of the remote server Variables variables always begin with a letter [A-Za-z] , can include underscores or numbers [0-9] the standard is to use all lowercase and avoid using numbers valid variable names include foo , foo_bar , foo_bar_5 , and fooBar invalid variable names include _foo , foo-bar , 5_foo_bar , foo.bar , foo bar in an inventory file a variable value is assigned using foo=bar in a playbook or variables include file, a variable value is assigned using foo: bar Playbook Variables variables can be passed via command line when calling ansible-playbook with --extra-vars option ansible-playbook example.yml --extra-vars \"foo=bar\" you can pass extra variables using quoted JSON, YAML or even by passing JSON or YAML file directly, like --extra-vars \"@even_more_vars.json\" or --extra-vars \"@even_more_vars.yml\" variables may be included inline with the rest of a playbook in a vars section --- - hosts: example vars: foo: bar tasks: # Prints \"Variable 'foo' is set to bar\" - debug: msg=\"Variable 'foo' is set to {{ foo }}\" variables may also be included in a separate file using vars_files section --- # Main playbook file` - hosts: example vars_files: - vars.yml tasks: - debug: msg=\"Variable 'foo' is set to {{ foo }}\" --- # Variables file 'vars.yml' in the same folder as the playbook foo: bar notice that when in a standalone file, vars are all at the root level of the YAML file variable files can also be imported conditionally, say for example on CentOS the apache service is named httpd and on Debian the apache service is called apache2 --- - hosts: example vars_files: - \"apache_default.yml\" - \"apache_{{ ansible_os_family }}.yml\" tasks: - service: name={{ apache }} state=running then add apache_CentOS.yml and apache_default.yml in the playbook folder, and define apache: httpd in CentOS file and apache: apache2 in the default file as long as you do not disable gather_facts (or if you run a setup task at some pont to gather facts manually), ansible stores the OS of the server in the variable ansible_os_family and will include the vars file with the resulting name if ansible cannot find the file with that name, it will use variables loaded from the first loaded file apache_default.yml Inventory variables variables can be added via inventory files, either inline with a host definition, or after a group # Host specific variables (defined inline) [washington] app1.example.com proxy_state=present app2.example.com proxy_state=absent # Variables defined for the entire group [washington:vars] cdn_host=washington.static.example.com api_version=3.0.1 ansible documentation reccomds not storing variables within the inventory file you can use group_vars and host_vars YAML variable files with a specific path and they will be assigned to hosts and groups defined if you want to apply a set of variables to host app1.example.com create a blank file named app1.example.com at /etc/ansible/host_vars/app1.example.com and add variables --- foo: bar bar: qux to apply a set of variables to the entire washington group, create a blank file at location /etc/ansible/group_vars/washington you can also put files in host_vars or group_vars directories in playbook directory, ansible will use variables defined in the in the /etc/ansible directory first, then those defined in playbook directories you can also use group_vars/all/ file that would apply to all groups Registeted Variables you can use register to store output of a particular command in a variable at runtime - name: Check list of running Node.js apps. command: forever list register: forever_list changed_when: false - name: Start example Node.js app. command: \"forever start {{ node_apps_location }}/app/app.js\" when: \"forever_list.stdout.find(node_apps_location + '/app/app.js') == -1\" Accessing Variables simple variables can be used as part of a task using syntax like {{ variable }} - command: /opt/my-app/rebuild {{ my_environment }} when command is run, contents of my_environment will be substituted so the resulting command will be something like /opt/my-app/rebuild dev if you define a list variable like: foo_list: - one - two - three you could access the first term in the array with either of foo[0] or foo|first the first one uses standard Python array syntax, the second one uses a filter provided by Jinja you can access any part of the array by drilling through array keys, either using bracket or dot sytax --- - hosts: localhost tasks: - debug: var=ansible_en0 PLAY [localhost] ********************************************************************************************************************************** TASK [Gathering Facts] **************************************************************************************************************************** ok: [localhost] TASK [debug] ************************************************************************************************************************************** ok: [localhost] => { \"ansible_en0\": { \"device\": \"en0\", \"flags\": [ \"UP\", \"BROADCAST\", \"SMART\", \"RUNNING\", \"SIMPLEX\", \"MULTICAST\" ], \"ipv4\": [ { \"address\": \"192.168.0.15\", \"broadcast\": \"192.168.0.255\", \"netmask\": \"255.255.255.0\", \"network\": \"192.168.0.0\" } ], \"ipv6\": [ { \"address\": \"fe80::1086:e144:3e2:9c56%en0\", \"prefix\": \"64\" } ], \"macaddress\": \"6c:96:cf:db:cb:4f\", \"media\": \"Unknown\", \"media_select\": \"autoselect\", \"mtu\": \"1500\", \"options\": [ \"PERFORMNUD\", \"DAD\" ], \"status\": \"active\", \"type\": \"ether\" } } PLAY RECAP **************************************************************************************************************************************** localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 now that you know the structure of the variable, you can access it using one of the following techniques {{ ansible_en0.ipv4.address }} {{ ansible_en0['ipv4']['address'] }} Magic variables with host and group variables and information if you need to retrieve a specific host variables from another host, uou can use magic hostvars variable which contains all the defined host variables (from inventory files and any discovered YAML files inside host_vars directories # From any host, returns \"jane\" {{ hostvars['host1']['admin_user'] other variables you may need to use from time to time groups - list of all group names in inventory group_names - list of all groups of which the current host is a part of inventory_hostname - hostname of current host according to inventory , can differ from ansible_hostname which is hostname reported by system inventory_hostname_short - first part of inventory_hostname up to first period play_hosts - all hosts on which current play will be run Note: you can read more at Magic Variables, and How To Access Information About Other Hosts Facts (Variables derived from system info) whenever you run a playbook, ansible first gathers information about each host in play facts cab gather information like host IP Addresses, CPU Type, disk space, operating system information, and network interface information to change when certain tasks are run or change certain information in config files to get a list of every gathered fact available you can use ansible <host> -m setup if you do not need to use facts and need to save a few seconds per host you can set gather_facts: no in your playbook - hosts: db gather_facts: no Note: if you have Facter or Ohai installed on a remote host, ansible will oalso include their facts prefixed by facter_ and ohai_ Note: when running playbooks against diff OSes, virtualization stacks or hosting providers, some facts may contian different information than what you are expecting Local Facts (Facts.d) another way of defining host-specific facts is to place a .fact file in a special directory on remote hosts /etc/ansible/facts.d/ these files can be either JSON or INI files, or you could use executables that return JSON example create `/etc/ansible/facts.d/settings.fact on a remote host [users] admin=jane,john normal=jim ansible nodejs -m setup -i hosts -a \"filter=ansible_local\" 192.168.55.55 | SUCCESS => { \"ansible_facts\": { \"ansible_local\": { \"settings\": { \"users\": { \"admin\": \"jane,john\", \"normal\": \"jim\" } } }, \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false } if part of a playbook adds a local .fact file, you can explicitly reload the local facts - name: Reload local facts setup: filter=ansible_local Ansible Vault - keeping secrets secret there are two primary ways to store sensitive data use a separate secret managment service such as Vault by HashiCorp, Keywhiz by square, or a hosted service like AWS Key Management Service or Azure Key Vault use Ansible Vault, built into Ansible and stores encrypted passwords and other sensitive data alongside playbook Ansible Vault works much like a real world vault any YAML file you would normally have in your playbook, (e.g. variables file, host vars, group vars, default vars, or even task includes) and store it in the vault encrypts the vault using a key (a password you set) store the key separately from the playbook in a location you control use the key to let ansible decrypt the vault whenever you run your playbook the following is a playbook that connects to a service API and requires a secure API key to do so --- - hosts: appserver vars_files: - vars/api_key.yml tasks: - name: Conect to service with our API key command: connect_to_service environment: SERVICE_API_KEY: \"{{ myapp_service_api_key }}\" the vars file which is stored alongside the playbook in plain text --- myapp_service_api_key: \"asfaASF9rqasfa92SDAA2ADAS\" it is not safe to store API keys in plain text, secrets should be encrypted to encrypt the file with Vault run the following ansible-vault encrypt api_key.yml enter passwords and the file should look like this $ANSIBLE_VAULT;1.1;AES256 66376632666135326131323637376436376466636361613266376530623731393535636532656338 3136306431353132316537393238363830356437303265650a623766373235303638633861346161 63646563356564636236376235386266393766653666323765626436386165376233636633323165 3734623462356566360a356465303238323333666134613161653832376461333666383833323933 64633539613631363935386639333962356637326639326431663131383732373965373038633038 31316230373664623333303537336436323633396635626362363338333265333865343930353433 613066653633643637363931346265376139 the next time you run playbook you need to provide password so that it can be decrypted in memory there are a number of ways you can provide the password depending on how you run the playbook you can provide the password at runtime when running playbook interactively ansible-playbook test.yml --ask-vault-pass you can edit the encrypted file with ansible-vault edit api_key.yml rekey - change a file password create - create a new file view - view an existing file decrypt - decrypt a file all these controls can be used with one or multiple files ansible-vault create x.yml y.yml z.yml for convenience or automated playbook runs you can supply vault password via a password file just like keys in ~/.ssh folder, you should set strict permissions 600 so that only you can read or write this file create the file ~/.ansible/vault_pass.txt with your password in it, set permissions to 600 and tell ansible the location of the file when you run the playbook ansible-playbook test.yml --vault-password-file ~/.ansible/vault_pass.txt you can also use an executable script (e.g. ~/.ansible/vault_pass.py ) with execute permissions 700 ) as long as the script outputs a single line of text, the vault password you can make vault operations slightly faster by installing python cryptography library pip install cryptography Variable Precedence --extra-vars passed in via the command line (these always win, no matter what). Task-level vars (in a task block). Block-level vars (for all tasks in a block). Role vars (e.g. [role]/vars/main.yml ) and vars from include_vars module. Vars set via set_facts modules. Vars set via register in a task. Individual play-level vars: 1. vars_files 2. vars_prompt 3. vars Host facts. Playbook host_vars . Playbook group_vars . Inventory: 1. host_vars 2. group_vars 3. vars Role default vars (e.g. [role]/defaults/main.yml ). Roles (to be discussed in the next chapter) should provide sane default values via the role defaults variables. These variables will be the fallback in case the variable is not defined anywhere else in the chain. Playbooks should rarely define variables (e.g. via set_fact ), but rather, variables should be defined either in included vars_files or, less often, via inventory. Only truly host- or group-specific variables should be defined in host or group inventories. Dynamic and static inventory sources should contain a minimum of variables, especially as these variables are often less visible to those maintaining a particular playbook. Command line variables (-e) should be avoided when possible. One of the main use cases is when doing local testing or running one-off playbooks where you aren't worried about the maintainability or idempotence of the tasks you're running. If/then/when - Conditionals some tasks use modules with built-in idempotence and you usually don't need to define conditional behaviour many tasks, especially command or shell modules , need input as to when they are supposed to run Jinja Expression, Python built-ins and Logic Jinja allows the definitions of literals like strings (\"string\") , integers ( 42 ), floats ( 42.33 ), lists ( [1, 2, 3] ), tuples (like lists but cannot be modified), dictionaries ( {key: value, key2: value2} ) and booleans ( true or false ) Jinja allows basic math operations, like addition, subtraction, multiplication, division, comparisions ( == , != , >= greater or equal to, etc), logical operartors and , or , not # the following expressions evaulate to 'true' 1 in [1, 2, 3] 'see' in 'Can you see me?' foo != bar (1 < 2) and ('a' not in 'best') # The following expressions evaluate to 'false': 4 in [1, 2, 3] foo == bar (foo != foo) or (a in [1, 2, 3]) Jinja also offers a set of 'tests' like foo is defined when Jinja does not provide enough power you can invoke Python built-in library like string.split and [number].is_signed() register any play can register a variable, and once registered it will be available to all subsequent tasks you can register the output of a shell command in a variable using the following syntax - shell: my_command_here register: my_command_result you can access stdout with my_command_result.stdout and stderr with my_command_result.stderr if you want to see diff properties of a registered variable, run a playbook with -v to inspect play output when - yum: name=mysql-server state=present when: is_db_server the above assumes you have defined is_db_server as a boolean earlier and will run the play if the value is true, or skip otherwise if you only define is_db_server on database servers, you could run tasks conditionally like so - yum: name=mysql-server state=present when: (is_db_server is defined) and is_db_server you can check the status of a running application and run a play only when application reports it is 'ready' in its output - command: my-app --status register: myapp_result - command: do-something-to-my-app when: \"'ready' in myapp_result.stdout\" the following are some real-world examples # From our Node.js playbook - register a command's output, then see # if the path to our app is in the output. Start the app if it's # not present. - command: forever list register: forever_list - command: forever start /path/to/app/app.js when: \"forever_list.stdout.find('/path/to/app/app.js') == -1\" # Run 'ping-hosts.sh' script if 'ping_hosts' variable is true. - command: /usr/local/bin/ping-hosts.sh when: ping_hosts # Run 'git-cleanup.sh' script if a branch we're interested in is # missing from git's list of branches in our project. - command: chdir=/path/to/project git branch register: git_branches - command: /path/to/project/scripts/git-cleanup.sh when: \"(is_app_server == true) and ('interesting-branch' not in \\ git_branches.stdout)\" # Downgrade PHP version if the current version contains '7.0'. - shell: php --version register: php_version - shell: yum -y downgrade php* when: \"'7.0' in php_version.stdout\" # Copy a file to the remote server if the hosts file doesn't exist. - stat: path=/etc/hosts register: hosts_file - copy: src=path/to/local/file dest=/path/to/remote/file when: hosts_file.stat.exists == false changed_when and failed_when it is difficult for ansible to determine if a given command results in changes if we use command or shell module without using changed_when ansible will always report a change most modules report whether they resulted in changes correctly, but you can override this behaviourby invoking changed_when # check if php composer install something or not - name: Install dependencies via Composer. command: \"/usr/local/bin/composer global require phpunit/phpunit --prefer-dist\" register: composer changed_when: \"'Nothing to install' not in composer.stdout\" many command-line utilities print results to stderr instead of stdout, so failed_when can be used to tell when a task has actually failed and is not reporting its results in the wrong way - name: Import a Jenkins job via CLI. shell: > java -jar /opt/jenkins-cli.jar -s http://localhost:8080/ create-job \"My Job\" < /usr/local/my-job.xml register: import failed_when: \"import.stderr and 'exists' not in import.stderr\" ignore_errors sometimes there are commands that should always run and they often report errors in these cases you can add ignore_errors: true to the task and it will remain unaware of any problems running with a particular task it is usually best to find a way to work with and around the errors genearted so that playbooks do fail if there are actual problems Delegation, Local Actions and Pauses some tasks, like sending a notification, communicating with load balancers, or making changes to DNS, networking or monitoring servers, require Ansible to run the task on the host machine (running the playbook) or another host besides the one being managed by the playbook you can delegate tasks to a particular host using delegate_to - name: Add server to Munin monitoring configuration. command: monitor-server webservers {{ inventory_hostname }} delegate_to: \"{{ monitoring_master }}\" delegation is often used to manage a server participation in a load balancer or replication pool you can either run a command locally or use one of Ansible built-in modules - name: Remove server from load balancer. command: remove-from-lb {{ inventory_hostname }} delegate_to: 127.0.0.1 if delegating to localhost, you can use shorthand local_action - name: Remove server from load balancer. local_action: command remove-from-lb {{ inventory_hostname }} Pausing playbook execution with wait_for you might also use local_action in the middle of a playbook to wait for a freshly-booted server or application to start listening on a port - name: Wait for web server to start. local_action: module: wait_for host: \"{{ inventory_hostname }}\" port: \"{{ webserver_port }}\" delay: 10 timeout: 300 state: started the above tasks wait until webserver_port is open on inventory_hostname as checked from the host running the ansible playbook with a 5-minute timeout (and 10 second check interval) wait_for can be used to pause for many different things Using host and port , wait a maximum of timeout seconds for the port to be available (or not). Using path (and search_regex if desired), wait a maximum of timeout seconds for the file to be present (or absent). Using host and port and drained for the state parameter, check if a given port has drained all it's active connections. Using delay , you can simply pause playbook execution for a given amount of time (in seconds). Running an entire playbook locally you can use --conection=local to speed up playbook execution by avoiding SSH connection overhead when you need to run a playbook locally --- - hosts: 127.0.0.1 gather_facts: no tasks: - name: Check the current system date. command: date register: date - name: Print the current system date. debug: var=date.stdout ansible-playbook test.yml --conection=local Prompts you can use vars_prompt to prompt for user input if there is no other way this information can be configured (e.g. environment variables, inventory variables, etc) --- - hosts: all vars_prompt: - name: share_user prompt: \"What is your network username?\" - name: share_pass prompt: \"What is your network password?\" private: yes there are a few special options you can add to prompts private : If set to yes , the user's input will be hidden on the command line. default : You can set a default value for the prompt, to save time for the end user. encrypt / confirm / salt_size : These values can be set for passwords so you can verify the entry (the user will have to enter the password twice if confirm is set to yes ), and encrypt it using a salt (with the specified size and crypt scheme). you should avoid prompts unless absolutely necessary Tags tags allow to run (or exclude) subsets of a playbook tasks you can tag roles, included files, individual tasks, and entire plays --- # You can apply tags to an entire play. - hosts: webservers tags: deploy roles: # Tags applied to a role will be applied to tasks in the role. - { role: tomcat, tags: ['tomcat', 'app'] } tasks: - name: Notify on completion. local_action: module: osx_say msg: \"{{inventory_hostname}} is finished!\" voice: Zarvox tags: - notifications - say - import_tasks: foo.yml tags: foo if we save the above playbook as tags.yml we can run the below command to only run tomcat role and the Notify on completion task ansible-playbook tags.yml --tags \"tmocat,say\" if we want to exclude anything tagged with notifications you can use --skip-tags ansible-playbook tags.yml --skip-tags \"notifications\" you can add tags with shorthand option tags: tagname but if adding more than one tag you have to use YAML list syntax # Shorthand list syntax. tags: ['one', 'two', 'three'] # Explicit list syntax. tags: - one - two - three # Non-working example. tags: one, two, three Blocks introduced in 2.0.0 blocks allow to group related tasks together and apply task parameters on the block level this allows to handle errors insider blocks in a way similar to exception handling the following is an example that uses blocks with when to run group of tasks without using when parameters on each task --- - hosts: web tasks: # Install and configure Apache on RHEL/CentOS hosts. - block: - yum: name=httpd state=present - template: src=httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf - service: name=httpd state=started enabled=yes when: ansible_os_family == 'RedHat' become: yes # Install and configure Apache on Debian/Ubuntu hosts. - block: - apt: name=apache2 state=present - template: src=httpd.conf.j2 dest=/etc/apache2/apache2.conf - service: name=apache2 state=started enabled=yes when: ansible_os_family == 'Debian' become: yes you can gracefully handle failures in certain tasks tasks: - block: - name: Script to connect the app to a monitoring service. script: monitoring-connect.sh rescue: - name: This will only run in case of an error in the block. debug: msg=\"There was an error in the block.\" always: - name: This will always run, no matter what. debug: msg=\"This always executes.\" Chapter 6 - Playbook Organization - Roles, Includes, and Imports Imports in the tasks section of your playbook you can add import_tasks directives to include tasks - import_tasks: imported-tasks.yml just like variable include file, tasks are formatted in a flat list in the included file --- - name: Add profile info for user. copy: src: example_profile dest: \"/home/{{ username }}/.profile\" owner: \"{{ username }}\" group: \"{{ username }}\" mode: 0744 - name: Add private keys for user. copy: src: \"{{ item.src }}\" dest: \"/home/{{ username }}/.ssh/{{ item.dest }}\" owner: \"{{ username }}\" group: \"{{ username }}\" mode: 0600 with_items: \"{{ ssh_private_keys }}\" - name: Restart example service. service: name=example state=restarted we used {{ username }} and {{ ssh_private_keys }} variables in this include file instead of hard-coded values so we coule make it reusable` you can define variables in you playbook inline variables, or an included file, but also by passing variables directlu into the includes using normal syntax - import_tasks: user.yml vars: username: johndoe ssh_private_keys: - { src: /path/to/johndoe/key1, dest: id_rsa } - { src: /path/to/johndoe/key2, dest: id_rsa_2 } - import_tasks: user.yml vars: username: janedoe ssh_private_keys: - { src: /path/to/janedoe/key1, dest: id_rsa } - { src: /path/to/janedoe/key2, dest: id_rsa_2 } imported files can even import other files Includes if you use import_tasks ansible statically imports the task file as if it were part of the main playbook, once, before the play is executed if you ned to have included tasks that are dynamic, i.e. they need to do different things depending on how the rest of the playbook runs, then you can use include_tasks take for example the following log_paths.yml - name: Check for existing log files in dynamic log_file_paths variable. find: paths: \"{{ item }}\" patterns: '*.log' register: found_log_file_paths with_items: \"{{ log_file_paths }}\" in this case the log_file_paths variable is set by a task earlier so this include file wouldn't be able to know the value of that vairable until the playbook has partly completed when this task file is included, it is done so dynamically - include_tasks: log_paths.yml Note: Early on, Ansible only had static include available for task inclusion, but as playbooks became more complex, people need to be able to include tasks that were processed when run (instead of added to the list of tasks before the play started running). So Ansible 2.1 introduced the static flag for include :. This worked, but overloaded the use of one keyword, so in Ansible 2.4, the use of include : was deprecated and you should use import_tasks if your tasks can basically be inlined before the playbook runs, or include_tasks if the tasks might need to be more dynamic (e.g. registering and reacting to a new registered variable). Dynamic includes until ansible 2.0 you could not use conditional includes as they were processed when your playbook run started after 2.0, it evaluates during playbook execution, so you could do something like the following # Include extra tasks file, only if it's present at runtime. - name: Check if extra_tasks.yml is present. stat: path=tasks/extra-tasks.yml register: extra_tasks_file connection: local - include_tasks: tasks/extra-tasks.yml when: extra_tasks_file.stat.exists if the file tasks/extra-tasks.yml is not present, it skips the include_tasks` you can even use a with_items loop with includes Handler inputs and includes handlers can be imported or included just like tasks, within the handlers section handlers: - import_tasks: handlers.yml Playbook imports playbooks can be included in other playbooks, using the same import syntax in the top level of playbook for playbooks you only have import_playbook available as they cannot be dynamic for example if you have two playbooks, you can use the following to run both at the same time - hosts: all remote_user: root tasks: [...] - import_playbook: web.yml - import_playbook: db.yml Complete includes example --- - hosts: all vars_files: - vars.yml pre_tasks: - name: Update apt cache if needed. apt: update_cache=yes cache_valid_time=3600 handlers: - import_tasks: handlers/handlers.yml tasks: - import_tasks: tasks/common.yml - import_tasks: tasks/apache.yml - import_tasks: tasks/php.yml - import_tasks: tasks/mysql.yml - import_tasks: tasks/composer.yml - import_tasks: tasks/drush.yml - import_tasks: tasks/drupal.yml --- # handlers/handlers.yml - name: restart apache service: name=apache2 state=restarted --- # tasks/drush.yml - name: Check out drush 8.x branch. git: repo: https://github.com/drush-ops/drush.git version: 8.x dest: /opt/drush - name: Install Drush dependencies with Composer.\" shell: > /usr/local/bin/composer install chdir=/opt/drush creates=/opt/drush/vendor/autoload.php - name: Create drush bin symlink. file: src: /opt/drush/drush dest: /usr/local/bin/drush state: link you can't use variables for task indlue file names when using import_tasks like you could with include_vars directives e.g. include_vars: \"{{ ansible_os_family }}.yml\" as a task or with vars_files you can use variables when using include_tasks Roles Role scaffolding instead of requiring you to explicitly include certain files and playbooks in a role, ansible automatically includes any main.yml files inside specific directories that make up the role there are only two directories required to make a working ansible role role_name \u251c\u2500\u2500 meta \u2514\u2500\u2500 tasks if you create a directory like the one above with a main.yml in each directory, ansible will run all the tasks defined in tasks/main.yml if you call the role from your playbook --- - hosts: all roles: - role_name your roles can live in the global role path /etc/ansible/ansible.cfg or a roles folder in the same directory as your main playbook you can also use ansible-galaxy init role_name to create an example role in the current working directory using init ensures the role is structured correctly in case you want to contribute the role to Ansible Galaxy Building your first role . \u251c\u2500\u2500 Vagrantfile \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 app.js \u2502 \u2514\u2500\u2500 package.json \u251c\u2500\u2500 playbook.yml \u2514\u2500\u2500 roles \u2514\u2500\u2500 nodejs \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 main.yml \u2514\u2500\u2500 tasks \u2514\u2500\u2500 main.yml 5 directories, 6 files Vagrantfile # -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = \"geerlingguy/centos7\" config.vm.network :private_network, ip: \"192.168.55.56\" config.ssh.insert_key = false config.vm.synced_folder \".\", \"/vagrant\", disabled: true config.vm.provider :virtualbox do |v| v.memory = 256 end # Ansible provisioner. config.vm.provision :ansible do |ansible| ansible.playbook = \"playbook.yml\" ansible.become = true end end app/app.js // Simple Express web server. // @see http://howtonode.org/getting-started-with-express // Load the express module. var express = require('express'); var app = express(); // Respond to requests for / with 'Hello World'. app.get('/', function(req, res){ res.send('Hello World!'); }); // Listen on port 80 (like a true web server). app.listen(80); console.log('Express server started successfully.'); app/package.json { \"name\": \"examplenodeapp\", \"description\": \"Example Express Node.js app.\", \"author\": \"Jeff Geerling <geerlingguy@mac.com>\", \"dependencies\": { \"express\": \"4.x\" }, \"engine\": \"node >= 0.10.6\" } playbook.yml --- - hosts: all vars: node_apps_location: /usr/local/opt/node pre_tasks: - name: Import Remi GPG key. rpm_key: key: \"https://rpms.remirepo.net/RPM-GPG-KEY-remi\" state: present - name: Install Remi repo. yum: name: \"https://rpms.remirepo.net/enterprise/remi-release-7.rpm\" state: present - name: Install EPEL repo. yum: name=epel-release state=present - name: Ensure firewalld is stopped (since this is a test server). service: name=firewalld state=stopped roles: - nodejs tasks: - name: Ensure Node.js app folder exists. file: \"path={{ node_apps_location }} state=directory\" - name: Copy example Node.js app to server. copy: \"src=app dest={{ node_apps_location }}\" - name: Install app dependencies defined in package.json. npm: \"path={{ node_apps_location }}/app\" - name: Check list of running Node.js apps. command: forever list register: forever_list changed_when: false - name: Start example Node.js app. command: \"forever start {{ node_apps_location }}/app/app.js\" when: \"forever_list.stdout.find(node_apps_location + '/app/app.js') == -1\" roles/nodejs/meta/main.yml --- dependencies: [] roles/nodejs/tasks/main.yml --- - name: Install Node.js (npm plus all its dependencies). yum: name=npm state=present enablerepo=epel - name: Install forever module (to run our Node.js app). npm: name=forever global=yes state=present More flexibility with role vars and defaults to make role more flexible you can make it use a lisrt of npm modules instrad of a hardcoded value, then allow playbooks to provide their own module list variable to override role default list when running role tasks, ansible picks up variables defined in role var/smain.yml and defaults/main.yml , but will aloow your playbook to override defaults or other role-provided variables modify tasks/main.yml to use a list variable --- - name: Install Node.js (npm plus all its dependencies). yum: name=npm state=present enablerepo=epel - name: Install npm modules required by our app. npm: name={{ item }} global=yes state=present with_items: \"{{ node_npm_modules }}\" let us provide a sane default for new node_npm_modules variable in defaults/main.yml: --- node_npm_modules: - forever to override this list we can create a new playbook and add a variable (either in vars secion or in an included file via vars_files node_npm_modules: - forever - async - request Other role parts: handlers, files, and templates Handlers in a prior example we added a handler to restart apache handlers: - name: restart apache service: name=apache2 state=restarted in roles, handlers are first-class citizens, and you can sotre handlers directly inside a main.yml file inside the role handlers directory handlers/main.yml --- - name: restart apache service: name=apache2 state=restarted you can call handler just like those included directly in your playbook notify: restart apache Files and Templates assume the role has been structured with files and templates inside files and templates directories roles/ example/ files/ example.conf meta/ main.yml templates/ example.xml.j2 tasks/ main.yml when copying a file directly to the server, add the filename or the full path from within a role's files directory, like so: - name: Copy configuration file to server directly. copy: src: example.conf dest: /etc/myapp/example.conf mode: 0644 Similarly, when specifying a template, add the filename or the full path from within a role's templates directory, like so: - name: Copy configuration file to server using a template. template: src: example.xml.j2 dest: /etc/myapp/example.xml mode: 0644","title":"Ansible For Devops"},{"location":"ansible_for_devops/#ansible-for-devops-notes","text":"git clone https://github.com/geerlingguy/ansible-for-devops.git","title":"Ansible for Devops notes"},{"location":"ansible_for_devops/#chapter-1-getting-started-with-ansible","text":"","title":"Chapter 1 - Getting Started with Ansible"},{"location":"ansible_for_devops/#installing-ansible","text":"","title":"Installing Ansible"},{"location":"ansible_for_devops/#mac","text":"brew install ansible","title":"Mac"},{"location":"ansible_for_devops/#mac-or-linux","text":"sudo pip install ansible","title":"Mac or Linux"},{"location":"ansible_for_devops/#upgrade-ansible-via-pip","text":"pip install --upgrade ansible","title":"upgrade ansible via pip"},{"location":"ansible_for_devops/#fedora-like-systems","text":"run the following to see if EPEL repo is already available yum repolist | grep epel If no results you need to install with the following commands rpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/\\ epel-release-6-8.noarch.rpm yum install epel-release yum -y install ansible","title":"fedora-like systems"},{"location":"ansible_for_devops/#debian-systems","text":"sudo apt-add-repository -y ppa:ansible/ansible sudo apt-get update sudo apt-get install -y ansible if missing add-apt-repository sudo apt-get install python-software-properties","title":"Debian systems"},{"location":"ansible_for_devops/#creating-a-basic-inventory-file","text":"ansible uses an inventory file to communicate with your servers create file at /etc/ansible/hosts and add one server to it sudo mkdir /etc/ansible sudo touch /etc/ansible/hosts edit hosts file and put the following in it [example] wwww.example.com since I am using vagrant for vm here is the hosts file I am using [example] ansible-devops-1 ansible_ssh_port=22 ansible_ssh_user=vagrant ansible_ssh_private_key_file=~/Workspace/tmp/ansible-devops/.vagrant/machines/default/virtualbox/private_key setting up server with vagrant Vagrant.configure(\"2\") do |config| config.vm.box = \"hashicorp/bionic64\" config.vm.hostname = \"ansible-devops-1\" config.vm.network \"private_network\", ip: \"192.168.50.101\" end","title":"Creating a basic inventory file"},{"location":"ansible_for_devops/#running-first-ad-hoc-ansible-command","text":"ansible example -m ping -u [username] Note: ansible assumes you are using passwordless (key-based) login for SSH You can read a primer on SSH/OpenSSH/keys here A useful command: ansible example -a \"free -m\" -u [username]","title":"Running first ad-hoc ansible command"},{"location":"ansible_for_devops/#chapter-2-local-infrastructure-development-ansible-and-vagrant","text":"","title":"Chapter 2 - Local Infrastructure Development: Ansible and Vagrant"},{"location":"ansible_for_devops/#setting-up-vagrant","text":"vagrant box add geerlingguy/centos7 vagrant init geerlingguy/centos7 vagrant up","title":"Setting up vagrant"},{"location":"ansible_for_devops/#using-ansible-with-vagrant","text":"Open Vagrant file and edit it Vagrant.configure(\"2\") do |config| config.vm.box = \"geerlingguy/centos7\" # Provisioning configuration for Ansible config.vm.provision \"ansible\" do |ansible| ansible.playbook = \"playbook.yml\" end end Create ansible playbook playbook.yml --- - hosts: all become: yes tasks: - name: Ensure NTP (for time synchronization) is installed. yum: name=ntp state=present - name: Ensure NTP is running service: name=ntpd state=started enabled=yes --- is a marker showing document is formatted in YAML - hosts: all tells ansible which hosts this playbook applies. all works here since Vagrant is invisibly usings its own Ansible inventory file become: yes since priviliged access is needed to install NTP and do system configs, this tells Ansible to use sudo for all tasks in playbook tasks: All tasks after this line will run on all hosts` the following command is equivalent of running yum install ntp but also checks if ntp is installed before - name: Ensure NTP (for time synchronization) is installed. yum: name=ntp state=present the following line checks and ensures ntpd service is started and running, and sets it to start at system boot - name: Ensure NTP is running service: name=ntpd state=started enabled=yes Note: you can leave out the name module but it is useful as it is a piece of documentation --- - hosts: all become: yes tasks: - yum: name=ntp state=present - service: name=ntpd state=started enabled=yes","title":"Using Ansible with vagrant"},{"location":"ansible_for_devops/#cleaning-up","text":"you can remove the vagrant machine from your system by running vagrant destroy if need be you can rebuild again using vagrant up","title":"Cleaning up"},{"location":"ansible_for_devops/#chapter-3-ad-hoc-commands","text":"","title":"Chapter 3 - Ad-Hoc Commands"},{"location":"ansible_for_devops/#build-infrastructure-with-vagrant-for-testing","text":"we are going to manage three VMs: two app servers and a database server # -*- mode: ruby -*- # vi: set ft=ruby : # VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| # General Vagrant VM configuration config.vm.box = \"geerlingguy/centos7\" config.ssh.insert_key = false config.vm.synced_folder \".\", \"/vagrant\", disabled: true config.vm.provider :virtualbox do |v| v.memory = 256 v.linked_clone = true end # Application server 1 config.vm.define \"app1\" do |app| app.vm.hostname = \"orc-app1.test\" app.vm.network :private_network, ip: \"192.168.60.4\" end # Application server 2 config.vm.define \"app2\" do |app| app.vm.hostname = \"orc-app2.test\" app.vm.network :private_network, ip: \"192.168.60.5\" end # Database server config.vm.define \"db\" do |db| db.vm.hostname = \"orc-db.test\" db.vm.network :private_network, ip: \"192.168.60.6\" end end # Application servers [app] 192.168.60.4 192.168.60.5 # Database server [db] 192.168.60.6 # Group 'multi' with all servers [multi:children] app db # Variables that will be applied to all servers [multi:vars] ansible_ssh_user=vagrant ansible_ssh_private_key_file=~/.vagrant.d/insecure_private_key the first block puts both of app servers into an app group the second block puts the database server into a db group the third block tells ansible to define a new group multi with child groups the fourth block adds variables to the multi group that will be applied to all servers within multi and all its children","title":"Build infrastructure with Vagrant for testing"},{"location":"ansible_for_devops/#your-first-adhoc-commands","text":"check vagrant configured VMs with right hostnames use -a argument hostname to run hostname on all servers ansible multi -a \"hostname\" by default ansible will run commands in parallel, using multiple process forks you can also tell ansible to run command using only one fork ansible multi -a \"hostname\" -f 1 check disk space avaiable ansible multi -a \"df -h\" check free memory ansible multi -a \"free -m\" check date and time ansible multi -a \"date\"","title":"Your first adhoc commands"},{"location":"ansible_for_devops/#make-changes-using-ansible-modules","text":"install NTP daemon on server to keep the time in sync ansible multi -b -m yum -a \"name=ntp state=present\" Note: -b option tells ansible to run command with become (i.e. run with sudo ) Note: if running command against server requiring password use -K so you can enter password make sure NTP daemon is started and set to run on boot ansible multi -b -m service -a \"name=ntpd state=started enabled=yes\" even if running shell commands you could wrap them in Ansible's shell or command modules ansible multi -m shell -a \"date\" check to make sure our servers are synced closely to official time in NTP server ansible multi -b -a \"service ntpd stop\" ansible multi -b -a \"ntpdate =q 0.rhel.pool.ntp.org\" ansible multi -b -a \"service ntpd start\"","title":"Make changes using Ansible modules"},{"location":"ansible_for_devops/#configure-application-servers","text":"ansible app -b -m yum -a \"name=MySQL-python state=present\" ansible app -b -m yum -a \"name=python-setuptools state=present\" ansible app -b -m easy_install -a \"name=django<2 state=present\" you could install django using pip , which can be install via easy_install Ansible's easy_install module doesn't allow you to uninstall packages lil pip does to make sure Django is installed and working correctly: ansible app -a \"python -c 'import django; \\ print django.get_version()'\"","title":"Configure application servers"},{"location":"ansible_for_devops/#configure-the-database-servers","text":"install mariadb, start it and configure firewall to allow access on 3306 ansible db -b -m yum -a \"name=mariadb-server state=present\" ansible db -b -m service -a \"name=mariadb state=started enabled=yes\" ansible db -b -a \"iptables -F\" ansible db -b -a \"iptables -A INPUT -s 192.168.60.0/24 -p tcp -m tcp --dport 3306 -j ACCEPT\" set up mariadb with access for one user ansible db -b -m yum -a \"name=MySQL-python state=present\" ansible db -b -m mysql_user -a \"name=django host=% password=12345 priv=*.*:ALL state=present\"","title":"Configure the Database servers"},{"location":"ansible_for_devops/#making-changes-to-just-one-server","text":"check service status and restart it on one server ansible app -b -a \"service ntpd status\" ansible app -b -a \"service ntpd restart\" --limit \"192.168.60.4\" we used the --limit argument to limit the command to a specific host in the specified group --limit will match either an exact string or a regular expression prefixed with ~ # Limit hosts with a simple pattern (asterisk is a wildcard) ansible app -b -a \"service ntpd restart\" --limit \"*.4\" # Limit hosts with a regular expression (prefix with a tilde) ansible app -b -a \"service ntpd restart\" --limit ~\".*\\.4\" Try to reserve the --limit option for running on single servers, if you often find yourself running on same set of server consider adding them to a group in your inventory","title":"Making changes to just one server"},{"location":"ansible_for_devops/#manages-users-and-groups","text":"ansible's users and group modules make user management easy and standard across linux flavors add an admin group on the app server for server admins ansible app -b -m group -a \"name=admin state=present\" you can remove a group by setting state=absent set a group id with gid=[gid] and indicate group is a system group with system=yes add user johndoe to app server with group just created and give him a home folder ansible app -b -m user -a \"name=johndoe group=admin createhome=yes\" if you want to create an SSH key for the new user you can use generate_ssh_key=yes you can also set UID of user uid=[uid] user's shell shell=[shell] and password password=[encrypted-password] if you want to delete an account: ansible app -b -m user -a \"name=johndoe state=absent remove=yes\"","title":"Manages users and groups"},{"location":"ansible_for_devops/#manage-packages","text":"ansible has a variety of package management modules for any flavor of linux there is also a generic package module that can be used for easier cross-platform ansible usage if you want to install a generic package like git on any system, you can use: ansible app -b -m package -a \"name=git state=present\"","title":"Manage packages"},{"location":"ansible_for_devops/#manage-files-and-directories","text":"","title":"Manage files and directories"},{"location":"ansible_for_devops/#get-information-about-a-file","text":"ansible multi -m stat -a \"path=/etc/environment\"","title":"Get information about a file"},{"location":"ansible_for_devops/#copy-a-file-to-the-servers","text":"while Ansible has more advanced file copy modules like rsync most file copy operations can be completed with Ansible's copy module ansible multi -m copy -a \"src=/etc/hosts dest=/tmp/hosts\" src can be a file or directory if you include a trailing slash, only contents of the directory will be copied into dest if you omit trailing slash, the contents and directory will be copied to dest copy module is perfect for single-file copies, and small directories if you want to copy hundreds of files, you should consider either copying an archive and expanding with unarchive module, or using synchronize or rysnc modules","title":"Copy a file to the servers"},{"location":"ansible_for_devops/#retrieve-a-file-from-servers","text":"fetch module works exactly like copy module but in reverse use the following command to grab hosts file from servers ansible multi -b -m fetch -a \"src=/etc/hosts dest=/tmp\" by default fetch will put the file from each server into a folder in the destination with the name of the host thus the server hosts file will end up in /tmp/192.168.60.6/etc/hosts if filenames are unique across servers you can use flat=yes and dest=/tmp/ to fetch directly into /tmp directory","title":"Retrieve a file from servers"},{"location":"ansible_for_devops/#create-directories-and-files","text":"file module used to create files and directories, manage permissions, and ownershoips, modify SELinux properties, and create symlinks here's how to create a directory: ansible multi -m file -a \"dest=/tmp/test mode=644 state=directory\" here's how to create a symlink ansible multi -m file -a \"src=/src/file dest=/dest/symlink state=link\"","title":"Create directories and files"},{"location":"ansible_for_devops/#delete-directories-and-files","text":"ansible multi -m file -a \"dest=/tmp/test state=absent\" other file-management modules include lineinfile , ini_file , and unarchive","title":"Delete directories and files"},{"location":"ansible_for_devops/#run-operations-in-the-background","text":"for long running operations (such as apt-get update ) you can tell Ansible to run commands asynchronously and poll servers to see when commands finish -B <seconds> the maximum amount of time to let job run -P <seconds> the amount of time to wait between polling the servers for updates Note: as of Ansible 2.0, async polling no longer displays output in real time, this is an open bug","title":"Run operations in the background"},{"location":"ansible_for_devops/#update-servers-asynchronously","text":"run yum -y update on all servers and set -P 0 to fire command on all server and print background job information ansible multi -b -B 3600 -P 0 -a \"yum -y update\" you can check the status using async_status module as long as you have ansible_job_id value to pass ansible multi -b -m async_status -as \"jid=[ansible_job_id]","title":"Update servers asynchronously"},{"location":"ansible_for_devops/#check-log-files","text":"common file operations like tail , cat , grep , etc... work through ansible command with a few caveats Operations that continously monitor a file won't work because Ansible only displays output after an operation is complete it's not a good idea to run a command that returns a huge amount of data if you redirect and filter output from a command you need to use shell module instead of default command module ansible multi -b -a \"tail /var/log/messages\" if you want to filter messages log with something like grep, use shell ansible multi -b -m shell -a \"tail /var/log/messages | grep ansible-command | wc -l\"","title":"Check log files"},{"location":"ansible_for_devops/#manage-cron-jobs","text":"you can manage cron jobs with the cron module` the following runs a shell script every day at 4 a.m ansible multi -b -m cron -a \"name='daily-cron-all-servers' hour=4 job='/path/to/daily-script.sh'\" ansible will assume <asterisk> for all values not specified valid values are day , hour , minute , month , and weekday you can also specify special time values like reboot , yearly , or monthly using `special_time=[value] you can set the user the job will run under via user=[user] you can create a backup of current crontab via backup=yes if you want to remove a corn job use state=absent ansible multi -b -m cron -a \"name='daily-cron-all-servers' state=absent\" you can also manage custom crontab files by specifing the location of the cron file with cron_file=cron_file_name","title":"Manage cron jobs"},{"location":"ansible_for_devops/#deploy-version-controlled-application","text":"for simple deployments, ad-hoc commands can help ansible app -b -m git -a \"repo=git://example.com/path/to/repo.git \\ dest=/opt/myapp update=yes version=1.2.4\" git module lets you specifiy branch, tag or even specific commit to force update checked-out copy we use update=yes if git is not installed you can use the following ansible app -b -m package -a \"name=git state=present\" if you get \"unkown hostkey\" add accept_hostkey=yes or add hostkey to you server's known_hosts run the application's update.sh shell script ansible app -b -a \"/opt/myapp/update.sh\"","title":"Deploy version-controlled application"},{"location":"ansible_for_devops/#ansibles-ssh-connection-history","text":"ansible uses standard and secure SSH connection to communicate with servers on thing that is universal to all of ansible's SSH conection methods is that it uses the connection to transfer one or a few files defining play or command to the remote server, runs the play/command, then deletes the transferred files and reports back the results","title":"Ansible's SSH connection history"},{"location":"ansible_for_devops/#faster-openssh-with-pipelining","text":"modern versions of Ansible allow you to improve performance of default OpenSSH implementation instead of copying files, running them on remote server, then removing them, the pipelining method will send and execute commands of most modules directly over SSH connection this can be enabled via pipelining=true under the ssh_connection section of the ansible config file ansible.cfg Note: you need to comment out Defaults requiretty option in /etc/sudoers for this to work well, it should be commented out by default but best to double check","title":"Faster OpenSSH with Pipelining"},{"location":"ansible_for_devops/#chapter-4-ansible-playbooks","text":"","title":"Chapter 4 - Ansible Playbooks"},{"location":"ansible_for_devops/#power-plays","text":"it is easy to convert shell scripts directly into ansible playbooks # Install apache yum install --quiet -y httpd httpd-devel # Copy configuration file cp httpd.conf /etc/httpd/conf/httpd.conf cp httpd-vhosts.conf /etc/httpd/conf/httpd-vhosts.conf # Start Apache and configure it to run at boot service httpd start chkconfig httpd on --- - hosts: all tasks: - name: Install Apache. command: yum install --quiet -y httpd httpd-devel - name: copy config file command: > cp httpd.conf /etc/httpd/conf/httpd.conf - command: > cp httpd-vhosts.conf /etc/httpd/conf/httpd-vhosts.conf - name: Start Apache and configure it to run at boot command: service httpd start - command: chkconfig httpd on to run the playbook you would call it using the following command ansible-playbook playbook.yml","title":"Power plays"},{"location":"ansible_for_devops/#revised-ansible-playbook-now-with-idempotency","text":"the above playbook will perform exactly like the shell script, but you can improve by using ansible's built-in modules --- - hosts: all become: yes tasks: - name: Install Apache yum: name: - httpd - httpd-devel state: present - name: Copy config files copy: src: \"{{ item.src }}\" dest: \"{{ item.dest }}\" owner: root group: root mode: 0644 with_items: - src: httpd.conf dest: /etc/httpd/conf/httpd.conf - src: httpd-vhosts.conf dest: /etc/httpd/conf/httpd-vhosts.conf - name: Make sure Apache is started now and at boot service: name=httpd state=started enabled=yes become: yes runs all commands through sudo we tell yum to make sure packages installed with state: present , but we could also use state: latest to ensure latest version or state: absent to make sure it packages not installed you can pass lists of variables to the tasks using with_items: and reference them using the item variable {{ item }} you can list as many variables as you want, even deeply-nested dicts this format of playbook is idempotent running the playbook with the --check option verifies the configuration matches waht's defined in the playbook without running the tasks on the server","title":"Revised Ansible Playbook - Now with idempotency!"},{"location":"ansible_for_devops/#running-playbooks","text":"running the above playbooks will run it across every host defined in your ansible inventory","title":"Running playbooks"},{"location":"ansible_for_devops/#limiting-playbooks-to-particular-hosts-and-groups","text":"you can limit a playbook by changing the hosts: definition the value can be set to all hosts, a group of hosts, multiple groups of hosts (e.g. webservers,dbservers , individual hosts (e.g. atl.example.com ), or a mixture of hosts you can do wild card matches like *.example.com you can also limit hosts via the following command ansible-playbook playbook.yml --limit webservers you can also limit playbook to one particular host ansible-playbook playbook.yml --limit xyz.example.com to see list of hosts that would be affected by your playbook before you run it: ansible-playbook playbook.yml --list-hosts playbook: playbook.yml play #1 (all): all TAGS: [] pattern: ['all'] hosts (3): 192.168.60.6 192.168.60.5 192.168.60.4","title":"Limiting playbooks to particular hosts and groups"},{"location":"ansible_for_devops/#setting-user-and-sudo-options","text":"if no remote_user is defined alongside hosts in a playbook, it assumes you'll connect as the user defined in your inventory file for a particular host, then it will fall back to your local user account name you can explicitly define a remote user with the --user (-u) option ansible-playbook playbook.yml --user=johndoe when you need to pass sudo password to remote server, use --ask-become-pass (-K) option you can force all tasks in playbook to use sudo with --become (-b) option you can define the sudo user for the tasks run via sudo (the default is root) with the --become-user (-U) option ansible-playbook playbook.yml --become --become-user=janedoe --ask-become-pass if you are not using key-based auth to connect to servers you can use --ask-pass","title":"Setting user and sudo options"},{"location":"ansible_for_devops/#other-options","text":"--inventory=PATH (-i PATH) define a custom inventory file (default is located at /etc/ansible/hosts --verbose (-v) verbose mode, you can pass -vvvv to give every detail --extra-vars=VARS (-e VARS) define variables to be used in the playook in \"key=value,key=value\" format --forks=NUM (-f NUM) set this number higher than 5 to increase the number of servers tasks will run on concurrently --connection=TYPE (-c TYPE) the type of connection which will be used, this defaults to ssh but you can use local` to run a playbook on your local machine, or on a remote server via cron --check run playbook in Check Mode ('Dry Run')","title":"Other options"},{"location":"ansible_for_devops/#real-world-playbook-centos-nodejs-app-server","text":"# install EPEL repo yum install -y epel-release # Import Remi GPG key wget https://rpms.remirepo.net/RPM-GPG-KEY-remi \\ -O /etc/pki/rpm-gpg/RPM-GPG-KEY-remi rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-remi # Install rpm -Uvh --quiet \\ https://rpms.remirepo.net/enterprise/remi-release-7.rpm # Install Node.js (npm plus all its dependencies) yum --enablerepo=epel -y install npm if you wanted to skip adding GPG keys, just run commands with --nogpgcheck in Ansible set the disable_gpg_check parameter of yum module to yes this is not a good idea, GPG stands for GNU Privacy Guard and it is a way for devs and package dists to sign their packages . \u251c\u2500\u2500 Vagrantfile \u2514\u2500\u2500 provisioning \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 app.js \u2502 \u2514\u2500\u2500 package.json \u2514\u2500\u2500 playbook.yml 2 directories, 4 files","title":"Real-World playbook: CentOS Node.js app server"},{"location":"ansible_for_devops/#vagrantfile","text":"# -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = \"geerlingguy/centos7\" config.vm.hostname = \"nodejs.test\" config.vm.network :private_network, ip: \"192.168.55.55\" config.ssh.insert_key = false config.vm.synced_folder \".\", \"/vagrant\", disabled: true config.vm.provider :virtualbox do |v| v.memory = 256 end # Ansible provisioner. config.vm.provision :ansible do |ansible| ansible.playbook = \"provisioning/playbook.yml\" end end","title":"Vagrantfile"},{"location":"ansible_for_devops/#provisioningplaybookyml","text":"--- - hosts: all become: yes vars: node_apps_location: /usr/local/opt/node tasks: - name: Install EPEL repo. yum: name=epel-release state=present - name: Import Remi GPG key. rpm_key: key: \"https://rpms.remirepo.net/RPM-GPG-KEY-remi\" state: present - name: Install Remi repo. yum: name: \"https://rpms.remirepo.net/enterprise/remi-release-7.rpm\" state: present - name: Ensure firewalld is stopped (since this is a test server). service: name=firewalld state=stopped - name: Install Node.js and npm. yum: name=npm state=present enablerepo=epel - name: Install Forever (to run our Node.js app). npm: name=forever global=yes state=present - name: Ensure Node.js app folder exists. file: \"path={{ node_apps_location }} state=directory\" - name: Copy example Node.js app to server. copy: \"src=app dest={{ node_apps_location }}\" - name: Install app dependencies defined in package.json. npm: \"path={{ node_apps_location }}/app\" - name: Check list of running Node.js apps. command: forever list register: forever_list changed_when: false - name: Start example Node.js app. command: \"forever start {{ node_apps_location }}/app/app.js\" when: \"forever_list.stdout.find(node_apps_location + '/app/app.js') == -1\"","title":"provisioning/playbook.yml"},{"location":"ansible_for_devops/#provisioningappappjs","text":"// Simple Express web server. // @see http://howtonode.org/getting-started-with-express // Load the express module. var express = require('express'); var app = express(); // Respond to requests for / with 'Hello World'. app.get('/', function(req, res){ res.send('Hello World!'); }); // Listen on port 80 (like a true web server). app.listen(80); console.log('Express server started successfully.');","title":"provisioning/app/app.js"},{"location":"ansible_for_devops/#provisioningapppackagejson","text":"{ \"name\": \"examplenodeapp\", \"description\": \"Example Express Node.js app.\", \"author\": \"Jeff Geerling <geerlingguy@mac.com>\", \"dependencies\": { \"express\": \"4.x\" }, \"engine\": \"node >= 0.10.6\" }","title":"provisioning/app/package.json"},{"location":"ansible_for_devops/#notes","text":"yum install EPEL repo (and automatically imports GPG key) rpm_key is a simple module that takes and imports RPM key from URL or file, or the key id of a key that is already present, and ensures the key is either present or absent firewall is disabled for testing purposes yum installs Node.js along with all required packages for npm , allows EPEL repo to be searched via enablerepo parameter (you can explicity disable a repo via disablerepo ) use npm module to install Node.js utility forever to launch app and keep running, setting global to yes tells NPM to install forever module in /usr/lib/node_modules/ Note: quotes are being used in YAML when there are Jinja variable (e.g. {{ var }} ) or when there are colons (:) in a string (e.g. URLs) register creates a new variable, forever_list , to be used in next play to determine when to run the play, it stashes the output (stdout, stderr) of defined command in variable passed to it changed_when tells when this play results in a change to the server, in this case forever list will ever change the server, so we just say false app is started using 'forever', we could start it by calling node {{ node_apps_location }}/app/app.js but we would not be able to control process easily and would also need to use nohup and & to avoid ansible hanging to avoid running multiple instances of node app, we start it using when and tell it to start only when the app's path is not in the forever list output","title":"notes"},{"location":"ansible_for_devops/#real-world-playbook-ubuntu-lamp-server-with-drupal","text":". \u251c\u2500\u2500 Vagrantfile \u2514\u2500\u2500 provisioning \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 playbook.yml \u251c\u2500\u2500 templates \u2502 \u2514\u2500\u2500 drupal.test.conf.j2 \u2514\u2500\u2500 vars.yml 2 directories, 5 files","title":"Real-world playbook: Ubuntu LAMP server with drupal"},{"location":"ansible_for_devops/#vagrantfile_1","text":"# -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = \"geerlingguy/ubuntu1604\" config.vm.network :private_network, ip: \"192.168.88.8\" config.vm.hostname = \"drupal.test\" config.ssh.insert_key = false config.vm.provider :virtualbox do |v| v.memory = 1024 end # Ansible provisioning. config.vm.provision \"ansible\" do |ansible| ansible.playbook = \"provisioning/playbook.yml\" ansible.config_file = \"provisioning/ansible.cfg\" ansible.verbose = \"vvvv\" end end","title":"Vagrantfile"},{"location":"ansible_for_devops/#provisioningansiblecfg","text":"[defaults] allow_world_readable_tmpfiles = true log_path = /var/log/ansible.log","title":"provisioning/ansible.cfg"},{"location":"ansible_for_devops/#provisioningplaybookyml_1","text":"--- - hosts: all become: yes vars_files: - vars.yml pre_tasks: - name: Update apt cache if needed. apt: update_cache=yes cache_valid_time=3600 handlers: - name: restart apache service: name=apache2 state=restarted tasks: - name: Get software for apt repository management. apt: state: present name: - python-apt - python-pycurl - name: Add ondrej repository for later versions of PHP. apt_repository: repo='ppa:ondrej/php' update_cache=yes - name: \"Install Apache, MySQL, PHP, and other dependencies.\" apt: state: present name: - git - curl - unzip - sendmail - apache2 - php7.1-common - php7.1-cli - php7.1-dev - php7.1-gd - php7.1-curl - php7.1-json - php7.1-opcache - php7.1-xml - php7.1-mbstring - php7.1-pdo - php7.1-mysql - php-apcu - libpcre3-dev - libapache2-mod-php7.1 - python-mysqldb - mysql-server - name: Disable the firewall (since this is for local dev only). service: name=ufw state=stopped - name: \"Start Apache, MySQL, and PHP.\" service: \"name={{ item }} state=started enabled=yes\" with_items: - apache2 - mysql - name: Enable Apache rewrite module (required for Drupal). apache2_module: name=rewrite state=present notify: restart apache - name: Add Apache virtualhost for Drupal 8. template: src: \"templates/drupal.test.conf.j2\" dest: \"/etc/apache2/sites-available/{{ domain }}.test.conf\" owner: root group: root mode: 0644 notify: restart apache - name: Symlink Drupal virtualhost to sites-enabled. file: src: \"/etc/apache2/sites-available/{{ domain }}.test.conf\" dest: \"/etc/apache2/sites-enabled/{{ domain }}.test.conf\" state: link notify: restart apache - name: Remove default virtualhost file. file: path: \"/etc/apache2/sites-enabled/000-default.conf\" state: absent notify: restart apache - name: Adjust OpCache memory setting. lineinfile: dest: \"/etc/php/7.1/apache2/conf.d/10-opcache.ini\" regexp: \"^opcache.memory_consumption\" line: \"opcache.memory_consumption = 96\" state: present notify: restart apache - name: Create a MySQL database for Drupal. mysql_db: \"db={{ domain }} state=present\" - name: Create a MySQL user for Drupal. mysql_user: name: \"{{ domain }}\" password: \"1234\" priv: \"{{ domain }}.*:ALL\" host: localhost state: present - name: Download Composer installer. get_url: url: https://getcomposer.org/installer dest: /tmp/composer-installer.php mode: 0755 - name: Run Composer installer. command: > php composer-installer.php chdir=/tmp creates=/usr/local/bin/composer - name: Move Composer into globally-accessible location. command: > mv /tmp/composer.phar /usr/local/bin/composer creates=/usr/local/bin/composer - name: Check out drush 8.x branch. git: repo: https://github.com/drush-ops/drush.git version: 8.x dest: /opt/drush - name: Install Drush dependencies with Composer. command: > /usr/local/bin/composer install chdir=/opt/drush creates=/opt/drush/vendor/autoload.php - name: Create drush bin symlink. file: src: /opt/drush/drush dest: /usr/local/bin/drush state: link - name: Check out Drupal Core to the Apache docroot. git: repo: https://git.drupal.org/project/drupal.git version: \"{{ drupal_core_version }}\" dest: \"{{ drupal_core_path }}\" register: git_checkout - name: Ensure Drupal codebase is owned by www-data. file: path: \"{{ drupal_core_path }}\" owner: www-data group: www-data recurse: true when: git_checkout.changed | bool - name: Install Drupal dependencies with Composer. command: > /usr/local/bin/composer install chdir={{ drupal_core_path }} creates={{ drupal_core_path }}/vendor/autoload.php become_user: www-data - name: Install Drupal. command: > drush si -y --site-name=\"{{ drupal_site_name }}\" --account-name=admin --account-pass=admin --db-url=mysql://{{ domain }}:1234@localhost/{{ domain }} --root={{ drupal_core_path }} creates={{ drupal_core_path }}/sites/default/settings.php notify: restart apache become_user: www-data","title":"provisioning/playbook.yml"},{"location":"ansible_for_devops/#provisioningvarsyml","text":"--- # The core version you want to use (e.g. 8.8.x, 8.9.x). drupal_core_version: \"8.8.x\" # The path where Drupal will be downloaded and installed. drupal_core_path: \"/var/www/drupal-{{ drupal_core_version }}-dev\" # The resulting domain will be [domain].test (with .test appended). domain: \"drupal\" # Your Drupal site name. drupal_site_name: \"Drupal Test\"","title":"provisioning/vars.yml"},{"location":"ansible_for_devops/#provisioningtemplatedrupaltestconfj2","text":"<VirtualHost *:80> ServerAdmin webmaster@localhost ServerName {{ domain }}.test ServerAlias www.{{ domain }}.test DocumentRoot {{ drupal_core_path }} <Directory \"{{ drupal_core_path }}\"> Options FollowSymLinks Indexes AllowOverride All </Directory> </VirtualHost>","title":"provisioning/template/drupal.test.conf.j2"},{"location":"ansible_for_devops/#notes_1","text":"you can run tasks before or after the main tasks (defined in tasks: ) or roles (defined in roles: ) using pre_tasks and post_tasks in this case we ensure apt cache is updated before rest of playbook, and we tell it to update cache if it's more than 3600 seconds (1 hour) since last update handlers are special kinds of tasks you run at the end of a play by adding notify option to any of the tasks in that group handlers will only be called if one of the tasks notifying the handler makes a change to the server (and doesn't fail), and it will be notified at the end of the play in this case, the handler has been defined to restart apache2 service after a configuration change just like variables, handlers and tasks may be placed in separate files to keep things tidy when a task fails, playbook execution is stopped and handlers aren't notified and triggered if oyou want to make sure handlers always run after a task uses notify even after failure, add --force-handlers in your ansible-playbook command python-apt and python-pycurl are helper libraries that allow python to manage apt more precisely, such as for apt_repository module linefile module ensures a particular line of text exists or not in a file command module is preferred option for running commands on a host and it works on most scenarios yet command does not run the command via remote shell /bin/sh so options like <, >, |, & and local environment vars like $HOME do not work shell allows you to pipe command output to other commands, access local environment, etc script executes shell scripts, though it is always better to use idempotent playbooks raw executes raw commands via ssh (should only be ever used as a last resort)","title":"notes"},{"location":"ansible_for_devops/#real-world-playbook-ubuntu-server-with-solr","text":". \u251c\u2500\u2500 Vagrantfile \u2514\u2500\u2500 provisioning \u251c\u2500\u2500 playbook.yml \u2514\u2500\u2500 vars.yml 1 directory, 3 files","title":"Real-world playbook: Ubuntu server with Solr"},{"location":"ansible_for_devops/#vagrantfile_2","text":"# -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = \"geerlingguy/ubuntu1604\" config.vm.network :private_network, ip: \"192.168.66.66\" config.vm.hostname = \"solr.test\" config.ssh.insert_key = false config.vm.provider :virtualbox do |v| v.memory = 1024 end # Ansible provisioner. config.vm.provision :ansible do |ansible| ansible.playbook = \"provisioning/playbook.yml\" end end","title":"Vagrantfile"},{"location":"ansible_for_devops/#provisioningplaybookyml_2","text":"--- - hosts: all become: true vars_files: - vars.yml pre_tasks: - name: Update apt cache if needed. apt: update_cache=true cache_valid_time=3600 tasks: - name: Install Java. apt: name=openjdk-8-jdk state=present - name: Download Solr. get_url: url: \"https://archive.apache.org/dist/lucene/solr/{{ solr_version }}/solr-{{ solr_version }}.tgz\" dest: \"{{ download_dir }}/solr-{{ solr_version }}.tgz\" checksum: \"{{ solr_checksum }}\" - name: Expand Solr. unarchive: src: \"{{ download_dir }}/solr-{{ solr_version }}.tgz\" dest: \"{{ download_dir }}\" remote_src: true creates: \"{{ download_dir }}/solr-{{ solr_version }}/README.txt\" - name: Run Solr installation script. command: > {{ download_dir }}/solr-{{ solr_version }}/bin/install_solr_service.sh {{ download_dir }}/solr-{{ solr_version }}.tgz -i /opt -d /var/solr -u solr -s solr -p 8983 creates={{ solr_dir }}/bin/solr - name: Ensure solr is started and enabled on boot. service: name=solr state=started enabled=yes","title":"provisioning/playbook.yml"},{"location":"ansible_for_devops/#provisioningvarsyml_1","text":"--- # The directory into which Solr will be downloaded for setup. download_dir: /tmp # The directory inside which Solr will be installed. solr_dir: /opt/solr # Solr version and download information. solr_version: 8.2.0 solr_checksum: sha512:beb4e37fc21bf483e3b6bae43cb06a49bc420a0f2b920c97909a69a5efeacba1e7d2ff09ae8018446c87bf007f88f06a59de73cd1923f0967e8206629b0509b6","title":"provisioning/vars.yml"},{"location":"ansible_for_devops/#notes_2","text":"when downloading files from remote servers, get_url module provides more flexibility than raw wget or curl commands use a full path to download file otherwise it will be re-downloaded on subsequent runs of the playbook you can use checksum to make sure it is the file you are expecting, if it doesn't match the file will be discarded when using unarchive we use creates option to make operation idempotent unarchive module docs show you can consolidate get_url and unarchive into one task, but since Solr installation requires original archive to be present we still need both tasks","title":"notes"},{"location":"ansible_for_devops/#chapter-5-ansible-playbook-beyond-the-basics","text":"","title":"Chapter 5 - Ansible Playbook - Beyond the Basics"},{"location":"ansible_for_devops/#handlers","text":"previously we used a simple handler to restart Apache, and tasks that affected Apache configs notified the handler with the option notify: restart apache handlers: - name: restart apache service: name=apache2 state=restarted tasks: - name: Enable Apache rewrite module apache2_module: name=rewrite state=present notify: restart apache if you want to notify multiple handlers from one task, use a list for notify option - name: Rebuild application configuration command: /opt/app/rebuild.sh notify: - restart apache - restart memcached you can have handlers notify other handlers by adding a notify option handlers: - name: restart apache service: name=apache2 state=restarted notify: restart memcached - name: restart memcached service: name=memcached state=restarted handlers will only run if a task notifies the handler handlers will run only once at the end of the play if you need to override run once behaviour, use meta: flush_handlers if play fails on a host before handlers are notified, handlers will never be run if you want handlers to run even when play has failed, you can use meta module or --force-handlers flag","title":"Handlers"},{"location":"ansible_for_devops/#environmental-variables","text":"there are multiple ways to work with env vars if you want to set env vars for remote user account you can add lines to user's .bash_profile - name: Add an environment variable to the remote user shell lineinfile: \"dest=~/.bash_profile regexp=^ENV_VAR= line=ENV_VAR=value\" all subsequent shell tasks will have access to this env variable, since only shell module understands shell commands that use env var to use an env var in further tasks, it's best to use a task register option to store env var in a variable Ansible can later use - name: Add an environment variable to the remote user shell lineinfile: \"dest=~/.bash_profile regexp=^ENV_VAR= line=ENV_VAR=value\" - name: Get the value of the environment variable we just added shell: 'source ~/.bash_profile && echo $ENV_VAR' register: foo - name: Print the value of the environment variable debug: msg=\"The variable is {{ foo.stdout }} we use source ~/.bash_profile to make sure we are using the latest env config for the remote user in some situations, the tasks all run over a persistent or semi-cached SSH session, over which $ENV_VAR would not yet be defined Note: there are many diff places you can store env vars, like .bashrc , .profile , and .bash_login in a user home folder. In this case we want the env var to be available to Ansible, which runs a pseudo-TTY shell session, in which case ~/.bash_profile is used to configure the environment. You can read more in Configuring your login sessions with dotfiles Linux will also read global env vars added to `/etc/environment so you can add your variable there - name: Add a global environment variable lineinfile: \"dest=/etc/environment regexp=^ENV_VAR= line=ENV_VAR=value\" become: yes if you require many env vars to be set, you might consider copy or template with a local file","title":"Environmental variables"},{"location":"ansible_for_devops/#pre-play-environment-variables","text":"you can set the environment for just one play using environment option for that play - name: Download a file using example-proxy as a proxy get_url: url=http://www.example.com/file.tar.gz dest=~/Downloads/ environment: http_proxy: http://example-proxy:80/ if you have many tasks that require a proxy, or some env var, you can pass them via your playbook vars section vars: proxy_vars: http_proxy: http://example-proxy:80/ https_proxy: https://example-proxy:443/ tasks: - name: Download a file using example-proxy as a proxy get_url: url=http://www.example.com/file.tar.gz dest=~/Downloads/ environment: proxy_vars if a proxy needs to be set system-wide you can do so using global /etc/environment file # In the 'vars' section of the playbook (set to 'absent' to disable proxy) proxy_state: present # in the 'tasks' section of the playbook - name: Configure the proxy lineinfile: dest: /etc/environment regexp: \"{{ item.regexp }}\" line: \"{{ item.line }}\" state: \"{{ proxy_state }}\" with_items: - regexp: \"^http_proxy=\" line: \"http_proxy=http://example-proxy:80/\" - regexp: \"^https_proxy=\" line: \"https_proxy=https://example-proxy:443/\" - regexp: \"^ftp_proxy=\" line: \"ftp_proxy=http://example-proxy:80/\" doing it this way allows us to configure whether the proxy is enabled per-server (using the proxy_state var) with one play Note: you can test remote env vars using the ansible command ansible test -m shell -a 'echo $TEST' . Be careful when doing so as using the wrong quotes you might end up printing a local env var instead of the remote server","title":"Pre-play environment variables"},{"location":"ansible_for_devops/#variables","text":"variables always begin with a letter [A-Za-z] , can include underscores or numbers [0-9] the standard is to use all lowercase and avoid using numbers valid variable names include foo , foo_bar , foo_bar_5 , and fooBar invalid variable names include _foo , foo-bar , 5_foo_bar , foo.bar , foo bar in an inventory file a variable value is assigned using foo=bar in a playbook or variables include file, a variable value is assigned using foo: bar","title":"Variables"},{"location":"ansible_for_devops/#playbook-variables","text":"variables can be passed via command line when calling ansible-playbook with --extra-vars option ansible-playbook example.yml --extra-vars \"foo=bar\" you can pass extra variables using quoted JSON, YAML or even by passing JSON or YAML file directly, like --extra-vars \"@even_more_vars.json\" or --extra-vars \"@even_more_vars.yml\" variables may be included inline with the rest of a playbook in a vars section --- - hosts: example vars: foo: bar tasks: # Prints \"Variable 'foo' is set to bar\" - debug: msg=\"Variable 'foo' is set to {{ foo }}\" variables may also be included in a separate file using vars_files section --- # Main playbook file` - hosts: example vars_files: - vars.yml tasks: - debug: msg=\"Variable 'foo' is set to {{ foo }}\" --- # Variables file 'vars.yml' in the same folder as the playbook foo: bar notice that when in a standalone file, vars are all at the root level of the YAML file variable files can also be imported conditionally, say for example on CentOS the apache service is named httpd and on Debian the apache service is called apache2 --- - hosts: example vars_files: - \"apache_default.yml\" - \"apache_{{ ansible_os_family }}.yml\" tasks: - service: name={{ apache }} state=running then add apache_CentOS.yml and apache_default.yml in the playbook folder, and define apache: httpd in CentOS file and apache: apache2 in the default file as long as you do not disable gather_facts (or if you run a setup task at some pont to gather facts manually), ansible stores the OS of the server in the variable ansible_os_family and will include the vars file with the resulting name if ansible cannot find the file with that name, it will use variables loaded from the first loaded file apache_default.yml","title":"Playbook Variables"},{"location":"ansible_for_devops/#inventory-variables","text":"variables can be added via inventory files, either inline with a host definition, or after a group # Host specific variables (defined inline) [washington] app1.example.com proxy_state=present app2.example.com proxy_state=absent # Variables defined for the entire group [washington:vars] cdn_host=washington.static.example.com api_version=3.0.1 ansible documentation reccomds not storing variables within the inventory file you can use group_vars and host_vars YAML variable files with a specific path and they will be assigned to hosts and groups defined if you want to apply a set of variables to host app1.example.com create a blank file named app1.example.com at /etc/ansible/host_vars/app1.example.com and add variables --- foo: bar bar: qux to apply a set of variables to the entire washington group, create a blank file at location /etc/ansible/group_vars/washington you can also put files in host_vars or group_vars directories in playbook directory, ansible will use variables defined in the in the /etc/ansible directory first, then those defined in playbook directories you can also use group_vars/all/ file that would apply to all groups","title":"Inventory variables"},{"location":"ansible_for_devops/#registeted-variables","text":"you can use register to store output of a particular command in a variable at runtime - name: Check list of running Node.js apps. command: forever list register: forever_list changed_when: false - name: Start example Node.js app. command: \"forever start {{ node_apps_location }}/app/app.js\" when: \"forever_list.stdout.find(node_apps_location + '/app/app.js') == -1\"","title":"Registeted Variables"},{"location":"ansible_for_devops/#accessing-variables","text":"simple variables can be used as part of a task using syntax like {{ variable }} - command: /opt/my-app/rebuild {{ my_environment }} when command is run, contents of my_environment will be substituted so the resulting command will be something like /opt/my-app/rebuild dev if you define a list variable like: foo_list: - one - two - three you could access the first term in the array with either of foo[0] or foo|first the first one uses standard Python array syntax, the second one uses a filter provided by Jinja you can access any part of the array by drilling through array keys, either using bracket or dot sytax --- - hosts: localhost tasks: - debug: var=ansible_en0 PLAY [localhost] ********************************************************************************************************************************** TASK [Gathering Facts] **************************************************************************************************************************** ok: [localhost] TASK [debug] ************************************************************************************************************************************** ok: [localhost] => { \"ansible_en0\": { \"device\": \"en0\", \"flags\": [ \"UP\", \"BROADCAST\", \"SMART\", \"RUNNING\", \"SIMPLEX\", \"MULTICAST\" ], \"ipv4\": [ { \"address\": \"192.168.0.15\", \"broadcast\": \"192.168.0.255\", \"netmask\": \"255.255.255.0\", \"network\": \"192.168.0.0\" } ], \"ipv6\": [ { \"address\": \"fe80::1086:e144:3e2:9c56%en0\", \"prefix\": \"64\" } ], \"macaddress\": \"6c:96:cf:db:cb:4f\", \"media\": \"Unknown\", \"media_select\": \"autoselect\", \"mtu\": \"1500\", \"options\": [ \"PERFORMNUD\", \"DAD\" ], \"status\": \"active\", \"type\": \"ether\" } } PLAY RECAP **************************************************************************************************************************************** localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 now that you know the structure of the variable, you can access it using one of the following techniques {{ ansible_en0.ipv4.address }} {{ ansible_en0['ipv4']['address'] }}","title":"Accessing Variables"},{"location":"ansible_for_devops/#magic-variables-with-host-and-group-variables-and-information","text":"if you need to retrieve a specific host variables from another host, uou can use magic hostvars variable which contains all the defined host variables (from inventory files and any discovered YAML files inside host_vars directories # From any host, returns \"jane\" {{ hostvars['host1']['admin_user'] other variables you may need to use from time to time groups - list of all group names in inventory group_names - list of all groups of which the current host is a part of inventory_hostname - hostname of current host according to inventory , can differ from ansible_hostname which is hostname reported by system inventory_hostname_short - first part of inventory_hostname up to first period play_hosts - all hosts on which current play will be run Note: you can read more at Magic Variables, and How To Access Information About Other Hosts","title":"Magic variables with host and group variables and information"},{"location":"ansible_for_devops/#facts-variables-derived-from-system-info","text":"whenever you run a playbook, ansible first gathers information about each host in play facts cab gather information like host IP Addresses, CPU Type, disk space, operating system information, and network interface information to change when certain tasks are run or change certain information in config files to get a list of every gathered fact available you can use ansible <host> -m setup if you do not need to use facts and need to save a few seconds per host you can set gather_facts: no in your playbook - hosts: db gather_facts: no Note: if you have Facter or Ohai installed on a remote host, ansible will oalso include their facts prefixed by facter_ and ohai_ Note: when running playbooks against diff OSes, virtualization stacks or hosting providers, some facts may contian different information than what you are expecting","title":"Facts (Variables derived from system info)"},{"location":"ansible_for_devops/#local-facts-factsd","text":"another way of defining host-specific facts is to place a .fact file in a special directory on remote hosts /etc/ansible/facts.d/ these files can be either JSON or INI files, or you could use executables that return JSON example create `/etc/ansible/facts.d/settings.fact on a remote host [users] admin=jane,john normal=jim ansible nodejs -m setup -i hosts -a \"filter=ansible_local\" 192.168.55.55 | SUCCESS => { \"ansible_facts\": { \"ansible_local\": { \"settings\": { \"users\": { \"admin\": \"jane,john\", \"normal\": \"jim\" } } }, \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false } if part of a playbook adds a local .fact file, you can explicitly reload the local facts - name: Reload local facts setup: filter=ansible_local","title":"Local Facts (Facts.d)"},{"location":"ansible_for_devops/#ansible-vault-keeping-secrets-secret","text":"there are two primary ways to store sensitive data use a separate secret managment service such as Vault by HashiCorp, Keywhiz by square, or a hosted service like AWS Key Management Service or Azure Key Vault use Ansible Vault, built into Ansible and stores encrypted passwords and other sensitive data alongside playbook Ansible Vault works much like a real world vault any YAML file you would normally have in your playbook, (e.g. variables file, host vars, group vars, default vars, or even task includes) and store it in the vault encrypts the vault using a key (a password you set) store the key separately from the playbook in a location you control use the key to let ansible decrypt the vault whenever you run your playbook the following is a playbook that connects to a service API and requires a secure API key to do so --- - hosts: appserver vars_files: - vars/api_key.yml tasks: - name: Conect to service with our API key command: connect_to_service environment: SERVICE_API_KEY: \"{{ myapp_service_api_key }}\" the vars file which is stored alongside the playbook in plain text --- myapp_service_api_key: \"asfaASF9rqasfa92SDAA2ADAS\" it is not safe to store API keys in plain text, secrets should be encrypted to encrypt the file with Vault run the following ansible-vault encrypt api_key.yml enter passwords and the file should look like this $ANSIBLE_VAULT;1.1;AES256 66376632666135326131323637376436376466636361613266376530623731393535636532656338 3136306431353132316537393238363830356437303265650a623766373235303638633861346161 63646563356564636236376235386266393766653666323765626436386165376233636633323165 3734623462356566360a356465303238323333666134613161653832376461333666383833323933 64633539613631363935386639333962356637326639326431663131383732373965373038633038 31316230373664623333303537336436323633396635626362363338333265333865343930353433 613066653633643637363931346265376139 the next time you run playbook you need to provide password so that it can be decrypted in memory there are a number of ways you can provide the password depending on how you run the playbook you can provide the password at runtime when running playbook interactively ansible-playbook test.yml --ask-vault-pass you can edit the encrypted file with ansible-vault edit api_key.yml rekey - change a file password create - create a new file view - view an existing file decrypt - decrypt a file all these controls can be used with one or multiple files ansible-vault create x.yml y.yml z.yml for convenience or automated playbook runs you can supply vault password via a password file just like keys in ~/.ssh folder, you should set strict permissions 600 so that only you can read or write this file create the file ~/.ansible/vault_pass.txt with your password in it, set permissions to 600 and tell ansible the location of the file when you run the playbook ansible-playbook test.yml --vault-password-file ~/.ansible/vault_pass.txt you can also use an executable script (e.g. ~/.ansible/vault_pass.py ) with execute permissions 700 ) as long as the script outputs a single line of text, the vault password you can make vault operations slightly faster by installing python cryptography library pip install cryptography","title":"Ansible Vault - keeping secrets secret"},{"location":"ansible_for_devops/#variable-precedence","text":"--extra-vars passed in via the command line (these always win, no matter what). Task-level vars (in a task block). Block-level vars (for all tasks in a block). Role vars (e.g. [role]/vars/main.yml ) and vars from include_vars module. Vars set via set_facts modules. Vars set via register in a task. Individual play-level vars: 1. vars_files 2. vars_prompt 3. vars Host facts. Playbook host_vars . Playbook group_vars . Inventory: 1. host_vars 2. group_vars 3. vars Role default vars (e.g. [role]/defaults/main.yml ). Roles (to be discussed in the next chapter) should provide sane default values via the role defaults variables. These variables will be the fallback in case the variable is not defined anywhere else in the chain. Playbooks should rarely define variables (e.g. via set_fact ), but rather, variables should be defined either in included vars_files or, less often, via inventory. Only truly host- or group-specific variables should be defined in host or group inventories. Dynamic and static inventory sources should contain a minimum of variables, especially as these variables are often less visible to those maintaining a particular playbook. Command line variables (-e) should be avoided when possible. One of the main use cases is when doing local testing or running one-off playbooks where you aren't worried about the maintainability or idempotence of the tasks you're running.","title":"Variable Precedence"},{"location":"ansible_for_devops/#ifthenwhen-conditionals","text":"some tasks use modules with built-in idempotence and you usually don't need to define conditional behaviour many tasks, especially command or shell modules , need input as to when they are supposed to run","title":"If/then/when - Conditionals"},{"location":"ansible_for_devops/#jinja-expression-python-built-ins-and-logic","text":"Jinja allows the definitions of literals like strings (\"string\") , integers ( 42 ), floats ( 42.33 ), lists ( [1, 2, 3] ), tuples (like lists but cannot be modified), dictionaries ( {key: value, key2: value2} ) and booleans ( true or false ) Jinja allows basic math operations, like addition, subtraction, multiplication, division, comparisions ( == , != , >= greater or equal to, etc), logical operartors and , or , not # the following expressions evaulate to 'true' 1 in [1, 2, 3] 'see' in 'Can you see me?' foo != bar (1 < 2) and ('a' not in 'best') # The following expressions evaluate to 'false': 4 in [1, 2, 3] foo == bar (foo != foo) or (a in [1, 2, 3]) Jinja also offers a set of 'tests' like foo is defined when Jinja does not provide enough power you can invoke Python built-in library like string.split and [number].is_signed()","title":"Jinja Expression, Python built-ins and Logic"},{"location":"ansible_for_devops/#register","text":"any play can register a variable, and once registered it will be available to all subsequent tasks you can register the output of a shell command in a variable using the following syntax - shell: my_command_here register: my_command_result you can access stdout with my_command_result.stdout and stderr with my_command_result.stderr if you want to see diff properties of a registered variable, run a playbook with -v to inspect play output","title":"register"},{"location":"ansible_for_devops/#when","text":"- yum: name=mysql-server state=present when: is_db_server the above assumes you have defined is_db_server as a boolean earlier and will run the play if the value is true, or skip otherwise if you only define is_db_server on database servers, you could run tasks conditionally like so - yum: name=mysql-server state=present when: (is_db_server is defined) and is_db_server you can check the status of a running application and run a play only when application reports it is 'ready' in its output - command: my-app --status register: myapp_result - command: do-something-to-my-app when: \"'ready' in myapp_result.stdout\" the following are some real-world examples # From our Node.js playbook - register a command's output, then see # if the path to our app is in the output. Start the app if it's # not present. - command: forever list register: forever_list - command: forever start /path/to/app/app.js when: \"forever_list.stdout.find('/path/to/app/app.js') == -1\" # Run 'ping-hosts.sh' script if 'ping_hosts' variable is true. - command: /usr/local/bin/ping-hosts.sh when: ping_hosts # Run 'git-cleanup.sh' script if a branch we're interested in is # missing from git's list of branches in our project. - command: chdir=/path/to/project git branch register: git_branches - command: /path/to/project/scripts/git-cleanup.sh when: \"(is_app_server == true) and ('interesting-branch' not in \\ git_branches.stdout)\" # Downgrade PHP version if the current version contains '7.0'. - shell: php --version register: php_version - shell: yum -y downgrade php* when: \"'7.0' in php_version.stdout\" # Copy a file to the remote server if the hosts file doesn't exist. - stat: path=/etc/hosts register: hosts_file - copy: src=path/to/local/file dest=/path/to/remote/file when: hosts_file.stat.exists == false","title":"when"},{"location":"ansible_for_devops/#changed_when-and-failed_when","text":"it is difficult for ansible to determine if a given command results in changes if we use command or shell module without using changed_when ansible will always report a change most modules report whether they resulted in changes correctly, but you can override this behaviourby invoking changed_when # check if php composer install something or not - name: Install dependencies via Composer. command: \"/usr/local/bin/composer global require phpunit/phpunit --prefer-dist\" register: composer changed_when: \"'Nothing to install' not in composer.stdout\" many command-line utilities print results to stderr instead of stdout, so failed_when can be used to tell when a task has actually failed and is not reporting its results in the wrong way - name: Import a Jenkins job via CLI. shell: > java -jar /opt/jenkins-cli.jar -s http://localhost:8080/ create-job \"My Job\" < /usr/local/my-job.xml register: import failed_when: \"import.stderr and 'exists' not in import.stderr\"","title":"changed_when and failed_when"},{"location":"ansible_for_devops/#ignore_errors","text":"sometimes there are commands that should always run and they often report errors in these cases you can add ignore_errors: true to the task and it will remain unaware of any problems running with a particular task it is usually best to find a way to work with and around the errors genearted so that playbooks do fail if there are actual problems","title":"ignore_errors"},{"location":"ansible_for_devops/#delegation-local-actions-and-pauses","text":"some tasks, like sending a notification, communicating with load balancers, or making changes to DNS, networking or monitoring servers, require Ansible to run the task on the host machine (running the playbook) or another host besides the one being managed by the playbook you can delegate tasks to a particular host using delegate_to - name: Add server to Munin monitoring configuration. command: monitor-server webservers {{ inventory_hostname }} delegate_to: \"{{ monitoring_master }}\" delegation is often used to manage a server participation in a load balancer or replication pool you can either run a command locally or use one of Ansible built-in modules - name: Remove server from load balancer. command: remove-from-lb {{ inventory_hostname }} delegate_to: 127.0.0.1 if delegating to localhost, you can use shorthand local_action - name: Remove server from load balancer. local_action: command remove-from-lb {{ inventory_hostname }}","title":"Delegation, Local Actions and Pauses"},{"location":"ansible_for_devops/#pausing-playbook-execution-with-wait_for","text":"you might also use local_action in the middle of a playbook to wait for a freshly-booted server or application to start listening on a port - name: Wait for web server to start. local_action: module: wait_for host: \"{{ inventory_hostname }}\" port: \"{{ webserver_port }}\" delay: 10 timeout: 300 state: started the above tasks wait until webserver_port is open on inventory_hostname as checked from the host running the ansible playbook with a 5-minute timeout (and 10 second check interval) wait_for can be used to pause for many different things Using host and port , wait a maximum of timeout seconds for the port to be available (or not). Using path (and search_regex if desired), wait a maximum of timeout seconds for the file to be present (or absent). Using host and port and drained for the state parameter, check if a given port has drained all it's active connections. Using delay , you can simply pause playbook execution for a given amount of time (in seconds).","title":"Pausing playbook execution with wait_for"},{"location":"ansible_for_devops/#running-an-entire-playbook-locally","text":"you can use --conection=local to speed up playbook execution by avoiding SSH connection overhead when you need to run a playbook locally --- - hosts: 127.0.0.1 gather_facts: no tasks: - name: Check the current system date. command: date register: date - name: Print the current system date. debug: var=date.stdout ansible-playbook test.yml --conection=local","title":"Running an entire playbook locally"},{"location":"ansible_for_devops/#prompts","text":"you can use vars_prompt to prompt for user input if there is no other way this information can be configured (e.g. environment variables, inventory variables, etc) --- - hosts: all vars_prompt: - name: share_user prompt: \"What is your network username?\" - name: share_pass prompt: \"What is your network password?\" private: yes there are a few special options you can add to prompts private : If set to yes , the user's input will be hidden on the command line. default : You can set a default value for the prompt, to save time for the end user. encrypt / confirm / salt_size : These values can be set for passwords so you can verify the entry (the user will have to enter the password twice if confirm is set to yes ), and encrypt it using a salt (with the specified size and crypt scheme). you should avoid prompts unless absolutely necessary","title":"Prompts"},{"location":"ansible_for_devops/#tags","text":"tags allow to run (or exclude) subsets of a playbook tasks you can tag roles, included files, individual tasks, and entire plays --- # You can apply tags to an entire play. - hosts: webservers tags: deploy roles: # Tags applied to a role will be applied to tasks in the role. - { role: tomcat, tags: ['tomcat', 'app'] } tasks: - name: Notify on completion. local_action: module: osx_say msg: \"{{inventory_hostname}} is finished!\" voice: Zarvox tags: - notifications - say - import_tasks: foo.yml tags: foo if we save the above playbook as tags.yml we can run the below command to only run tomcat role and the Notify on completion task ansible-playbook tags.yml --tags \"tmocat,say\" if we want to exclude anything tagged with notifications you can use --skip-tags ansible-playbook tags.yml --skip-tags \"notifications\" you can add tags with shorthand option tags: tagname but if adding more than one tag you have to use YAML list syntax # Shorthand list syntax. tags: ['one', 'two', 'three'] # Explicit list syntax. tags: - one - two - three # Non-working example. tags: one, two, three","title":"Tags"},{"location":"ansible_for_devops/#blocks","text":"introduced in 2.0.0 blocks allow to group related tasks together and apply task parameters on the block level this allows to handle errors insider blocks in a way similar to exception handling the following is an example that uses blocks with when to run group of tasks without using when parameters on each task --- - hosts: web tasks: # Install and configure Apache on RHEL/CentOS hosts. - block: - yum: name=httpd state=present - template: src=httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf - service: name=httpd state=started enabled=yes when: ansible_os_family == 'RedHat' become: yes # Install and configure Apache on Debian/Ubuntu hosts. - block: - apt: name=apache2 state=present - template: src=httpd.conf.j2 dest=/etc/apache2/apache2.conf - service: name=apache2 state=started enabled=yes when: ansible_os_family == 'Debian' become: yes you can gracefully handle failures in certain tasks tasks: - block: - name: Script to connect the app to a monitoring service. script: monitoring-connect.sh rescue: - name: This will only run in case of an error in the block. debug: msg=\"There was an error in the block.\" always: - name: This will always run, no matter what. debug: msg=\"This always executes.\"","title":"Blocks"},{"location":"ansible_for_devops/#chapter-6-playbook-organization-roles-includes-and-imports","text":"","title":"Chapter 6 - Playbook Organization - Roles, Includes, and Imports"},{"location":"ansible_for_devops/#imports","text":"in the tasks section of your playbook you can add import_tasks directives to include tasks - import_tasks: imported-tasks.yml just like variable include file, tasks are formatted in a flat list in the included file --- - name: Add profile info for user. copy: src: example_profile dest: \"/home/{{ username }}/.profile\" owner: \"{{ username }}\" group: \"{{ username }}\" mode: 0744 - name: Add private keys for user. copy: src: \"{{ item.src }}\" dest: \"/home/{{ username }}/.ssh/{{ item.dest }}\" owner: \"{{ username }}\" group: \"{{ username }}\" mode: 0600 with_items: \"{{ ssh_private_keys }}\" - name: Restart example service. service: name=example state=restarted we used {{ username }} and {{ ssh_private_keys }} variables in this include file instead of hard-coded values so we coule make it reusable` you can define variables in you playbook inline variables, or an included file, but also by passing variables directlu into the includes using normal syntax - import_tasks: user.yml vars: username: johndoe ssh_private_keys: - { src: /path/to/johndoe/key1, dest: id_rsa } - { src: /path/to/johndoe/key2, dest: id_rsa_2 } - import_tasks: user.yml vars: username: janedoe ssh_private_keys: - { src: /path/to/janedoe/key1, dest: id_rsa } - { src: /path/to/janedoe/key2, dest: id_rsa_2 } imported files can even import other files","title":"Imports"},{"location":"ansible_for_devops/#includes","text":"if you use import_tasks ansible statically imports the task file as if it were part of the main playbook, once, before the play is executed if you ned to have included tasks that are dynamic, i.e. they need to do different things depending on how the rest of the playbook runs, then you can use include_tasks take for example the following log_paths.yml - name: Check for existing log files in dynamic log_file_paths variable. find: paths: \"{{ item }}\" patterns: '*.log' register: found_log_file_paths with_items: \"{{ log_file_paths }}\" in this case the log_file_paths variable is set by a task earlier so this include file wouldn't be able to know the value of that vairable until the playbook has partly completed when this task file is included, it is done so dynamically - include_tasks: log_paths.yml Note: Early on, Ansible only had static include available for task inclusion, but as playbooks became more complex, people need to be able to include tasks that were processed when run (instead of added to the list of tasks before the play started running). So Ansible 2.1 introduced the static flag for include :. This worked, but overloaded the use of one keyword, so in Ansible 2.4, the use of include : was deprecated and you should use import_tasks if your tasks can basically be inlined before the playbook runs, or include_tasks if the tasks might need to be more dynamic (e.g. registering and reacting to a new registered variable).","title":"Includes"},{"location":"ansible_for_devops/#dynamic-includes","text":"until ansible 2.0 you could not use conditional includes as they were processed when your playbook run started after 2.0, it evaluates during playbook execution, so you could do something like the following # Include extra tasks file, only if it's present at runtime. - name: Check if extra_tasks.yml is present. stat: path=tasks/extra-tasks.yml register: extra_tasks_file connection: local - include_tasks: tasks/extra-tasks.yml when: extra_tasks_file.stat.exists if the file tasks/extra-tasks.yml is not present, it skips the include_tasks` you can even use a with_items loop with includes","title":"Dynamic includes"},{"location":"ansible_for_devops/#handler-inputs-and-includes","text":"handlers can be imported or included just like tasks, within the handlers section handlers: - import_tasks: handlers.yml","title":"Handler inputs and includes"},{"location":"ansible_for_devops/#playbook-imports","text":"playbooks can be included in other playbooks, using the same import syntax in the top level of playbook for playbooks you only have import_playbook available as they cannot be dynamic for example if you have two playbooks, you can use the following to run both at the same time - hosts: all remote_user: root tasks: [...] - import_playbook: web.yml - import_playbook: db.yml","title":"Playbook imports"},{"location":"ansible_for_devops/#complete-includes-example","text":"--- - hosts: all vars_files: - vars.yml pre_tasks: - name: Update apt cache if needed. apt: update_cache=yes cache_valid_time=3600 handlers: - import_tasks: handlers/handlers.yml tasks: - import_tasks: tasks/common.yml - import_tasks: tasks/apache.yml - import_tasks: tasks/php.yml - import_tasks: tasks/mysql.yml - import_tasks: tasks/composer.yml - import_tasks: tasks/drush.yml - import_tasks: tasks/drupal.yml --- # handlers/handlers.yml - name: restart apache service: name=apache2 state=restarted --- # tasks/drush.yml - name: Check out drush 8.x branch. git: repo: https://github.com/drush-ops/drush.git version: 8.x dest: /opt/drush - name: Install Drush dependencies with Composer.\" shell: > /usr/local/bin/composer install chdir=/opt/drush creates=/opt/drush/vendor/autoload.php - name: Create drush bin symlink. file: src: /opt/drush/drush dest: /usr/local/bin/drush state: link you can't use variables for task indlue file names when using import_tasks like you could with include_vars directives e.g. include_vars: \"{{ ansible_os_family }}.yml\" as a task or with vars_files you can use variables when using include_tasks","title":"Complete includes example"},{"location":"ansible_for_devops/#roles","text":"","title":"Roles"},{"location":"ansible_for_devops/#role-scaffolding","text":"instead of requiring you to explicitly include certain files and playbooks in a role, ansible automatically includes any main.yml files inside specific directories that make up the role there are only two directories required to make a working ansible role role_name \u251c\u2500\u2500 meta \u2514\u2500\u2500 tasks if you create a directory like the one above with a main.yml in each directory, ansible will run all the tasks defined in tasks/main.yml if you call the role from your playbook --- - hosts: all roles: - role_name your roles can live in the global role path /etc/ansible/ansible.cfg or a roles folder in the same directory as your main playbook you can also use ansible-galaxy init role_name to create an example role in the current working directory using init ensures the role is structured correctly in case you want to contribute the role to Ansible Galaxy","title":"Role scaffolding"},{"location":"ansible_for_devops/#building-your-first-role","text":". \u251c\u2500\u2500 Vagrantfile \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 app.js \u2502 \u2514\u2500\u2500 package.json \u251c\u2500\u2500 playbook.yml \u2514\u2500\u2500 roles \u2514\u2500\u2500 nodejs \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 main.yml \u2514\u2500\u2500 tasks \u2514\u2500\u2500 main.yml 5 directories, 6 files","title":"Building your first role"},{"location":"ansible_for_devops/#vagrantfile_3","text":"# -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = \"geerlingguy/centos7\" config.vm.network :private_network, ip: \"192.168.55.56\" config.ssh.insert_key = false config.vm.synced_folder \".\", \"/vagrant\", disabled: true config.vm.provider :virtualbox do |v| v.memory = 256 end # Ansible provisioner. config.vm.provision :ansible do |ansible| ansible.playbook = \"playbook.yml\" ansible.become = true end end","title":"Vagrantfile"},{"location":"ansible_for_devops/#appappjs","text":"// Simple Express web server. // @see http://howtonode.org/getting-started-with-express // Load the express module. var express = require('express'); var app = express(); // Respond to requests for / with 'Hello World'. app.get('/', function(req, res){ res.send('Hello World!'); }); // Listen on port 80 (like a true web server). app.listen(80); console.log('Express server started successfully.');","title":"app/app.js"},{"location":"ansible_for_devops/#apppackagejson","text":"{ \"name\": \"examplenodeapp\", \"description\": \"Example Express Node.js app.\", \"author\": \"Jeff Geerling <geerlingguy@mac.com>\", \"dependencies\": { \"express\": \"4.x\" }, \"engine\": \"node >= 0.10.6\" }","title":"app/package.json"},{"location":"ansible_for_devops/#playbookyml","text":"--- - hosts: all vars: node_apps_location: /usr/local/opt/node pre_tasks: - name: Import Remi GPG key. rpm_key: key: \"https://rpms.remirepo.net/RPM-GPG-KEY-remi\" state: present - name: Install Remi repo. yum: name: \"https://rpms.remirepo.net/enterprise/remi-release-7.rpm\" state: present - name: Install EPEL repo. yum: name=epel-release state=present - name: Ensure firewalld is stopped (since this is a test server). service: name=firewalld state=stopped roles: - nodejs tasks: - name: Ensure Node.js app folder exists. file: \"path={{ node_apps_location }} state=directory\" - name: Copy example Node.js app to server. copy: \"src=app dest={{ node_apps_location }}\" - name: Install app dependencies defined in package.json. npm: \"path={{ node_apps_location }}/app\" - name: Check list of running Node.js apps. command: forever list register: forever_list changed_when: false - name: Start example Node.js app. command: \"forever start {{ node_apps_location }}/app/app.js\" when: \"forever_list.stdout.find(node_apps_location + '/app/app.js') == -1\"","title":"playbook.yml"},{"location":"ansible_for_devops/#rolesnodejsmetamainyml","text":"--- dependencies: []","title":"roles/nodejs/meta/main.yml"},{"location":"ansible_for_devops/#rolesnodejstasksmainyml","text":"--- - name: Install Node.js (npm plus all its dependencies). yum: name=npm state=present enablerepo=epel - name: Install forever module (to run our Node.js app). npm: name=forever global=yes state=present","title":"roles/nodejs/tasks/main.yml"},{"location":"ansible_for_devops/#more-flexibility-with-role-vars-and-defaults","text":"to make role more flexible you can make it use a lisrt of npm modules instrad of a hardcoded value, then allow playbooks to provide their own module list variable to override role default list when running role tasks, ansible picks up variables defined in role var/smain.yml and defaults/main.yml , but will aloow your playbook to override defaults or other role-provided variables modify tasks/main.yml to use a list variable --- - name: Install Node.js (npm plus all its dependencies). yum: name=npm state=present enablerepo=epel - name: Install npm modules required by our app. npm: name={{ item }} global=yes state=present with_items: \"{{ node_npm_modules }}\" let us provide a sane default for new node_npm_modules variable in defaults/main.yml: --- node_npm_modules: - forever to override this list we can create a new playbook and add a variable (either in vars secion or in an included file via vars_files node_npm_modules: - forever - async - request","title":"More flexibility with role vars and defaults"},{"location":"ansible_for_devops/#other-role-parts-handlers-files-and-templates","text":"","title":"Other role parts: handlers, files, and templates"},{"location":"ansible_for_devops/#handlers_1","text":"in a prior example we added a handler to restart apache handlers: - name: restart apache service: name=apache2 state=restarted in roles, handlers are first-class citizens, and you can sotre handlers directly inside a main.yml file inside the role handlers directory handlers/main.yml --- - name: restart apache service: name=apache2 state=restarted you can call handler just like those included directly in your playbook notify: restart apache","title":"Handlers"},{"location":"ansible_for_devops/#files-and-templates","text":"assume the role has been structured with files and templates inside files and templates directories roles/ example/ files/ example.conf meta/ main.yml templates/ example.xml.j2 tasks/ main.yml when copying a file directly to the server, add the filename or the full path from within a role's files directory, like so: - name: Copy configuration file to server directly. copy: src: example.conf dest: /etc/myapp/example.conf mode: 0644 Similarly, when specifying a template, add the filename or the full path from within a role's templates directory, like so: - name: Copy configuration file to server using a template. template: src: example.xml.j2 dest: /etc/myapp/example.xml mode: 0644","title":"Files and Templates"},{"location":"cheatsheet/","text":"Cheatsheet vim commands :tabnew - new tab :tabnext - move to next tab :tabprev - move to previous tab :tabonly - close all tabs except for current one :tabclose - close current tab and all its windows :tabdo <command> - run command on all tabs :noh - turn off highlight until next search :echo @% - show relative path of current open file :%s/foo/bar/gci - find each occurence of foo, in every line, and replace it with bar, case insensitive, ask for confirmation :put =strftime('%c') - put date timestamp in editor :%!python -m json.tool key maps <C-f> - page forward <C-b> - page backward gt - tab next gT - tab previous #gt - move to tab number # <C-w> s - split <C-w> v - split window vertically <C-w> w - switch split window <C-w> <direction> - move to pane in <direction> direction J - joins line the cursor is on with below line ciw'<C-r>\"' - delete and yank word, add quotes, note that cw works when start of word, ciw works at any pos in word di'hPl2x - unquote word <C-r> \" - insert contents of \" register tmux commands tmux new-session -s <session-name> tmux attach-session -t <session-name> tmux list-sessions tmux commands move-window -t # - move window to num swapt-window -s # -t # - swap window from source to destination num setw synchronize-panes - write to all panes key maps <C-b> <M-<direction>> - resize pane in <direction> direction <C-b> <direction> - move to pane in <direction> direction <C-b> [ - enter copy mode <C-b> z - zoom pane toggle <C-b> q - show pane numbers <C-b> q# - move to # pane <C-b> <space> - toggle between layouts <C-b> % - split pane vertical <C-b> <double-quote> - split pane horizontal <C-b> n - next window <C-b> p - previous window <C-b> & - close current window <C-b> c - create new window <C-b> , - rename current window <C-b> # - move to # window <C-b> s - list all sessions <C-b> $ - rename session <C-b> ( - previous session <C-b> ) - next session <C-b> : - command prompt git commands # get remote url git config --get remote.origin.url # list untracked files git ls-files --others --exclude-standard Rebase forked repo # Add the remote, call it \"upstream\": git remote add upstream https://github.com/whoever/whatever.git # Fetch all the branches of that remote into remote-tracking branches, such as upstream/master: git fetch upstream # Make sure that you're on your master branch: git checkout master # Rewrite your master branch so that any commits of yours that aren't already in upstream/master are replayed on top of that other branch: git rebase upstream/master # If you don't want to rewrite the history of your master branch, (for example because other people may have cloned it) then you should replace the last command with git merge upstream/master. However, for making further pull requests that are as clean as possible, it's probably better to rebase. # If you've rebased your branch onto upstream/master you may need to force the push in order to push it to your own forked repository on GitHub. You'd do that with: git push -f origin master apt install neovim sudo add-apt-repository ppa:neovim-ppa/stable sudo apt-get update mkdocs install pip install mkdocs getting started mkdocs new my-project cd my-project mkdocs serve Vagrant configs config.vm.network \"private_network\", ip: \"192.168.50.4\" config.vm.hostname (string) commands vagrant ssh-config | grep IdentityFile - get ssh identity file path vagrant plugin install vagrant-hostupdater vagrant global-status - get state of all active Vagrant envs on system vagrant global-status --prune - clear cache to remove stale results VirtualBox Install # Linux mint 19 wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add - echo \"deb [arch=amd64] http://download.virtualbox.org/virtualbox/debian bionic contrib\" | sudo tee /etc/apt/sources.list.d/virtualbox.list sudo apt-get update sudo apt-get install -y virtualbox-6.0 Linux Utils commands Push folders to remote server # The -a option is a combination flag. It stands for \u201carchive\u201d and syncs # recursively and preserves symbolic links, special and device files, # modification times, group, owner, and permissions. It is more commonly used # than -r and is usually what you want to use. # # If you are transferring files that have not already been compressed, like # text files, you can reduce the network transfer by adding compression with # the -z option # # The -P flag is very helpful. It combines the flags \u2013progress and \u2013partial. # The first of these gives you a progress bar for the transfers and the # second allows you to resume interrupted transfers rsync -azP ~/dir1 username@remote_host:destination_directory # boot directly into console mode sudo systemctl set-default multi-user.target # get current systemd target sudo systemctl get-default # get all current active targets sudo systemctl list-units --type target --state active # boot into graphical mode sudo systemctl set-default graphical.target Connect to wifi via command line # Connect to wifi using command line option 1 just edit /etc/network/interfaces and write: auto wlan0 iface wlan0 inet dhcp wpa-ssid {ssid} wpa-psk {password} # After that write and close file and use command: sudo dhclient wlan0 # Option 2 Provided you replace your Wireless network card, Wi-Fi Network name, and Wi-FI Password this should also work. # First, get your WiFi card up and running: sudo ifconfig wlan0 up # Now scan for a list of WiFi networks in range: sudo iwlist wlan0 scan # This will show you a list of wireless networks, pick yours from the list: sudo iwconfig wlan0 essid Wifi2Home key s:ABCDE12345 # To obtain the IP address, now request it with the Dynamic Host Client: sudo dhclient wlan0 # You should then be connected to the WiFi network. The first option is better, because it will be able to run as a cron job to start up the wifi whenever you need it going. If you need to turn off your WiFi for whatever reason, just type: sudo ifconfig wlan0 down Change default gateway # get routes route -n # change default gateway route del default gw <default_gateway_ip> route add default gw <default_gateway_ip> Ignore lid closed To disable entering the sleep mode I had to edit the /etc/systemd/logind.conf file and modify the line: #HandleLidSwitch=suspend to HandleLidSwitch=ignore Then do sudo service systemd-logind restart Turn on keyboard backlight To enable the backlight: echo 2 | sudo tee /sys/class/leds/asus::kbd_backlight/brightness The 2 at echo 2 | can be changed to a value between 0 - 3, with 3 being the brightest. To disable the backlight, enter: echo 0 | sudo tee /sys/class/leds/asus::kbd_backlight/brightness The path may vary depending on laptop model and your OS. For example Lenovo Thinkpad L390 running Manjaro has /sys/class/leds/tpacpi::kbd_backlight/brightness. You can use find to see the correct path: find /sys/class/leds -name '*kbd_backlight' Remove ssh entry ssh-keygen -f \"/home/<USER_NAME>/.ssh/known_hosts\" -R \"192.168.0.120\" Format and bulk load json jq '.[]' data.json -rc | while read line; do echo \"{ \\\"index\\\" : { \\\"_index\\\" : \\\"books\\\" } }\\n$line\"; done > data-proc.json split -l 100000 data-proc.json data-part- for filename in data-part-*; do curl -s -H \"Content-Type: application/x-ndjson\" -XPOST 192.168.0.133:9200/_bulk --data-binary \"@$filename\"; sleep 1; done jq -r '.Data[] | [(.GameId | tostring), .Name, .Vendor] | join(\", \")' response.json | sort -n > response.csv Fix locale issue sudo locale-gen en_US en_US.UTF-8 sudo dpkg-reconfigure locales ansible commands ansible <HOST_GROUP> -m ping ansible-playbook basic-setup.yml --limit laptop_vivo -K --extra-vars \"user=<USER_NAME>\" ansible -i inventories/hosts laptops -m \"setup\" ansible -i inventories/hosts laptops -m \"ping\" ansible-playbook basic-setup.yml -i inventories/hosts --limit laptop_vivo -K ansible-galaxy init <role-name> php snippets file_put_contents('/home/deploy/debug_logs.txt', date(\"D M d, Y G:i\") . ' -> ' . json_encode($registrationData) . PHP_EOL , FILE_APPEND | LOCK_EX); notes ansible update trusted certificates link mitmproxy transparent link mitmproxy --mode transparent --showhost mitmproxy Methods of adding proxy certificate Ansible CentOS - hosts: rizk_dev become: yes vars: local_cert_path: ./certs/ rizk_dev_cert_path: /etc/pki/ca-trust/source/anchors/ handlers: - name: restart php-fpm service: name: php-fpm state: restarted tasks: - name: install ca package on rhel systems yum: name: ca-certificates state: present - name: enable dynamic ca configuration on rhel6 shell: /bin/update-ca-trust enable - name: push mitmproxy certificates synchronize: src: \"{{ local_cert_path }}\" dest: \"{{ rizk_dev_cert_path }}\" - name: file: path: \"{{ rizk_dev_cert_path }}\" state: directory recurse: true owner: root group: root mode: 0644 - name: update trusted ca redhat shell: /bin/update-ca-trust register: update_ca_trust - name: Add rizk-dev to hosts file lineinfile: path: \"/home/{{ ansible_user }}/.bashrc\" regexp: \"{{ item.regexp }}\" line: \"{{ item.line }}\" become_user: \"{{ ansible_user }}\" with_items: - regexp: \"^alias\\\\sset-default-gateway\" line: \"alias set-default-gateway='sudo route del default gw 192.168.29.100 && sudo route add default gw 10.0.2.2'\" - regexp: \"^alias\\\\sset-mitmproxy-gateway\" line: \"alias set-mitmproxy-gateway='sudo route del default gw 10.0.2.2 && sudo route add default gw 192.168.29.100'\" - regexp: \"^alias\\\\srestart-docker\" line: \"alias restart-dockr='sudo systemctl restart docker'\" - name: Enable proxy for php-fpm lineinfile: path: \"/etc/php-fpm.d/www.conf\" regexp: \"{{ item.regexp }}\" line: \"{{ item.line }}\" with_items: - regexp: \"^env\\\\[HTTP_PROXY\\\\]\" line: \"env[HTTP_PROXY] = 192.168.29.100:8080\" - regexp: \"^env\\\\[HTTPS_PROXY\\\\]\" line: \"env[HTTPS_PROXY] = 192.168.29.100:8080\" tags: [never, enable-proxy] notify: restart php-fpm - name: Disbale proxy for php-fpm lineinfile: path: \"/etc/php-fpm.d/www.conf\" regexp: \"{{ item }}\" state: absent loop: - \"^env\\\\[HTTP_PROXY\\\\]\" - \"^env\\\\[HTTPS_PROXY\\\\]\" tags: [never, disable-proxy] notify: restart php-fpm Ubuntu - name: push mitmproxy certificates synchronize: src: mitmproxy-certs/mitmproxy-ca-cert.pem dest: /usr/local/share/ca-certificates/mitmproxy-ca-cert.crt - name: file: path: /usr/local/share/ca-certificates state: directory recurse: true owner: root group: root mode: 0644 - name: Update trusted ca shell: /usr/sbin/update-ca-certificates register: update_ca - debug: msg: \"{{ update_ca }}\" Docker Alpine FROM node:12.13.1-alpine3.10 AS builder COPY ./package.json /app/package.json COPY ./.env.development.local /app/.env.development.local COPY ./backend /app/backend WORKDIR /app RUN [\"apk\", \"add\", \"--update\", \"nodejs\", \"npm\"] RUN [\"npm\", \"install\"] FROM node:12.13.1-alpine3.10 WORKDIR /app COPY --from=builder /app /app RUN apk update && apk add ca-certificates && rm -rf /var/cache/apk/* COPY ./mitmproxy-ca-cert.crt /usr/local/share/ca-certificates/mitmproxy-ca-cert.crt RUN update-ca-certificates CMD [\"npm\", \"run\", \"server:dev\"] version: \"3.6\" services: backend: build: context: ./ dockerfile: Dev.Backend.Dockerfile ports: - 8181:8181 volumes: - ./backend:/app/backend environment: - NODE_TLS_REJECT_UNAUTHORIZED=0 - HTTP_PROXY=http://192.168.29.200:8080 - HTTPS_PROXY=http://192.168.29.200:8080 frontend: build: context: ./ dockerfile: Dev.Frontend.Dockerfile ports: - 3000:3000 volumes: - ./frontend:/app/frontend heroku commands heroku -v heroku apps --team=betsson-zecure heroku builds -a source-of-wealth-staging heroku config --app=source-of-wealth-staging heroku container:login heroku container:push web --app=source-of-wealth-staging heroku container:release web --app=source-of-wealth-staging heroku login heroku logs --app=source-of-wealth-staging heroku plugins:install heroku-builds heroku ps -a source-of-wealth-staging heroku ps:kill release.7483 -a source-of-wealth-staging docker commands docker build -t source-of-wealth:latest . docker run --name source-of-wealth -d -p 3000:3000 --env-file ./.env source-of-wealth docker container stop source-of-wealth docker container rm source-of-wealth docker exec -it source-of-wealth /bin/sh ssh # generate ssh private / public key ssh-keygen -t rsa -b 2048 -C \"email@example.com\" # copy public key to clipboard xclip -sel clip < ~/.ssh/id_rsa.pub # test wether ssh key added correctly to github ssh -T git@github.com","title":"Cheatsheet"},{"location":"cheatsheet/#cheatsheet","text":"","title":"Cheatsheet"},{"location":"cheatsheet/#vim","text":"","title":"vim"},{"location":"cheatsheet/#commands","text":":tabnew - new tab :tabnext - move to next tab :tabprev - move to previous tab :tabonly - close all tabs except for current one :tabclose - close current tab and all its windows :tabdo <command> - run command on all tabs :noh - turn off highlight until next search :echo @% - show relative path of current open file :%s/foo/bar/gci - find each occurence of foo, in every line, and replace it with bar, case insensitive, ask for confirmation :put =strftime('%c') - put date timestamp in editor :%!python -m json.tool","title":"commands"},{"location":"cheatsheet/#key-maps","text":"<C-f> - page forward <C-b> - page backward gt - tab next gT - tab previous #gt - move to tab number # <C-w> s - split <C-w> v - split window vertically <C-w> w - switch split window <C-w> <direction> - move to pane in <direction> direction J - joins line the cursor is on with below line ciw'<C-r>\"' - delete and yank word, add quotes, note that cw works when start of word, ciw works at any pos in word di'hPl2x - unquote word <C-r> \" - insert contents of \" register","title":"key maps"},{"location":"cheatsheet/#tmux","text":"","title":"tmux"},{"location":"cheatsheet/#commands_1","text":"tmux new-session -s <session-name> tmux attach-session -t <session-name> tmux list-sessions","title":"commands"},{"location":"cheatsheet/#tmux-commands","text":"move-window -t # - move window to num swapt-window -s # -t # - swap window from source to destination num setw synchronize-panes - write to all panes","title":"tmux commands"},{"location":"cheatsheet/#key-maps_1","text":"<C-b> <M-<direction>> - resize pane in <direction> direction <C-b> <direction> - move to pane in <direction> direction <C-b> [ - enter copy mode <C-b> z - zoom pane toggle <C-b> q - show pane numbers <C-b> q# - move to # pane <C-b> <space> - toggle between layouts <C-b> % - split pane vertical <C-b> <double-quote> - split pane horizontal <C-b> n - next window <C-b> p - previous window <C-b> & - close current window <C-b> c - create new window <C-b> , - rename current window <C-b> # - move to # window <C-b> s - list all sessions <C-b> $ - rename session <C-b> ( - previous session <C-b> ) - next session <C-b> : - command prompt","title":"key maps"},{"location":"cheatsheet/#git","text":"","title":"git"},{"location":"cheatsheet/#commands_2","text":"# get remote url git config --get remote.origin.url # list untracked files git ls-files --others --exclude-standard","title":"commands"},{"location":"cheatsheet/#rebase-forked-repo","text":"# Add the remote, call it \"upstream\": git remote add upstream https://github.com/whoever/whatever.git # Fetch all the branches of that remote into remote-tracking branches, such as upstream/master: git fetch upstream # Make sure that you're on your master branch: git checkout master # Rewrite your master branch so that any commits of yours that aren't already in upstream/master are replayed on top of that other branch: git rebase upstream/master # If you don't want to rewrite the history of your master branch, (for example because other people may have cloned it) then you should replace the last command with git merge upstream/master. However, for making further pull requests that are as clean as possible, it's probably better to rebase. # If you've rebased your branch onto upstream/master you may need to force the push in order to push it to your own forked repository on GitHub. You'd do that with: git push -f origin master","title":"Rebase forked repo"},{"location":"cheatsheet/#apt","text":"","title":"apt"},{"location":"cheatsheet/#install","text":"","title":"install"},{"location":"cheatsheet/#neovim","text":"sudo add-apt-repository ppa:neovim-ppa/stable sudo apt-get update","title":"neovim"},{"location":"cheatsheet/#mkdocs","text":"","title":"mkdocs"},{"location":"cheatsheet/#install_1","text":"pip install mkdocs","title":"install"},{"location":"cheatsheet/#getting-started","text":"mkdocs new my-project cd my-project mkdocs serve","title":"getting started"},{"location":"cheatsheet/#vagrant","text":"","title":"Vagrant"},{"location":"cheatsheet/#configs","text":"config.vm.network \"private_network\", ip: \"192.168.50.4\" config.vm.hostname (string)","title":"configs"},{"location":"cheatsheet/#commands_3","text":"vagrant ssh-config | grep IdentityFile - get ssh identity file path vagrant plugin install vagrant-hostupdater vagrant global-status - get state of all active Vagrant envs on system vagrant global-status --prune - clear cache to remove stale results","title":"commands"},{"location":"cheatsheet/#virtualbox","text":"","title":"VirtualBox"},{"location":"cheatsheet/#install_2","text":"# Linux mint 19 wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add - echo \"deb [arch=amd64] http://download.virtualbox.org/virtualbox/debian bionic contrib\" | sudo tee /etc/apt/sources.list.d/virtualbox.list sudo apt-get update sudo apt-get install -y virtualbox-6.0","title":"Install"},{"location":"cheatsheet/#linux-utils","text":"","title":"Linux Utils"},{"location":"cheatsheet/#commands_4","text":"","title":"commands"},{"location":"cheatsheet/#push-folders-to-remote-server","text":"# The -a option is a combination flag. It stands for \u201carchive\u201d and syncs # recursively and preserves symbolic links, special and device files, # modification times, group, owner, and permissions. It is more commonly used # than -r and is usually what you want to use. # # If you are transferring files that have not already been compressed, like # text files, you can reduce the network transfer by adding compression with # the -z option # # The -P flag is very helpful. It combines the flags \u2013progress and \u2013partial. # The first of these gives you a progress bar for the transfers and the # second allows you to resume interrupted transfers rsync -azP ~/dir1 username@remote_host:destination_directory # boot directly into console mode sudo systemctl set-default multi-user.target # get current systemd target sudo systemctl get-default # get all current active targets sudo systemctl list-units --type target --state active # boot into graphical mode sudo systemctl set-default graphical.target","title":"Push folders to remote server"},{"location":"cheatsheet/#connect-to-wifi-via-command-line","text":"# Connect to wifi using command line option 1 just edit /etc/network/interfaces and write: auto wlan0 iface wlan0 inet dhcp wpa-ssid {ssid} wpa-psk {password} # After that write and close file and use command: sudo dhclient wlan0 # Option 2 Provided you replace your Wireless network card, Wi-Fi Network name, and Wi-FI Password this should also work. # First, get your WiFi card up and running: sudo ifconfig wlan0 up # Now scan for a list of WiFi networks in range: sudo iwlist wlan0 scan # This will show you a list of wireless networks, pick yours from the list: sudo iwconfig wlan0 essid Wifi2Home key s:ABCDE12345 # To obtain the IP address, now request it with the Dynamic Host Client: sudo dhclient wlan0 # You should then be connected to the WiFi network. The first option is better, because it will be able to run as a cron job to start up the wifi whenever you need it going. If you need to turn off your WiFi for whatever reason, just type: sudo ifconfig wlan0 down","title":"Connect to wifi via command line"},{"location":"cheatsheet/#change-default-gateway","text":"# get routes route -n # change default gateway route del default gw <default_gateway_ip> route add default gw <default_gateway_ip>","title":"Change default gateway"},{"location":"cheatsheet/#ignore-lid-closed","text":"To disable entering the sleep mode I had to edit the /etc/systemd/logind.conf file and modify the line: #HandleLidSwitch=suspend to HandleLidSwitch=ignore Then do sudo service systemd-logind restart","title":"Ignore lid closed"},{"location":"cheatsheet/#turn-on-keyboard-backlight","text":"To enable the backlight: echo 2 | sudo tee /sys/class/leds/asus::kbd_backlight/brightness The 2 at echo 2 | can be changed to a value between 0 - 3, with 3 being the brightest. To disable the backlight, enter: echo 0 | sudo tee /sys/class/leds/asus::kbd_backlight/brightness The path may vary depending on laptop model and your OS. For example Lenovo Thinkpad L390 running Manjaro has /sys/class/leds/tpacpi::kbd_backlight/brightness. You can use find to see the correct path: find /sys/class/leds -name '*kbd_backlight'","title":"Turn on keyboard backlight"},{"location":"cheatsheet/#remove-ssh-entry","text":"ssh-keygen -f \"/home/<USER_NAME>/.ssh/known_hosts\" -R \"192.168.0.120\"","title":"Remove ssh entry"},{"location":"cheatsheet/#format-and-bulk-load-json","text":"jq '.[]' data.json -rc | while read line; do echo \"{ \\\"index\\\" : { \\\"_index\\\" : \\\"books\\\" } }\\n$line\"; done > data-proc.json split -l 100000 data-proc.json data-part- for filename in data-part-*; do curl -s -H \"Content-Type: application/x-ndjson\" -XPOST 192.168.0.133:9200/_bulk --data-binary \"@$filename\"; sleep 1; done jq -r '.Data[] | [(.GameId | tostring), .Name, .Vendor] | join(\", \")' response.json | sort -n > response.csv","title":"Format and bulk load json"},{"location":"cheatsheet/#fix-locale-issue","text":"sudo locale-gen en_US en_US.UTF-8 sudo dpkg-reconfigure locales","title":"Fix locale issue"},{"location":"cheatsheet/#ansible","text":"","title":"ansible"},{"location":"cheatsheet/#commands_5","text":"ansible <HOST_GROUP> -m ping ansible-playbook basic-setup.yml --limit laptop_vivo -K --extra-vars \"user=<USER_NAME>\" ansible -i inventories/hosts laptops -m \"setup\" ansible -i inventories/hosts laptops -m \"ping\" ansible-playbook basic-setup.yml -i inventories/hosts --limit laptop_vivo -K ansible-galaxy init <role-name>","title":"commands"},{"location":"cheatsheet/#php","text":"","title":"php"},{"location":"cheatsheet/#snippets","text":"file_put_contents('/home/deploy/debug_logs.txt', date(\"D M d, Y G:i\") . ' -> ' . json_encode($registrationData) . PHP_EOL , FILE_APPEND | LOCK_EX);","title":"snippets"},{"location":"cheatsheet/#notes","text":"ansible update trusted certificates link mitmproxy transparent link mitmproxy --mode transparent --showhost","title":"notes"},{"location":"cheatsheet/#mitmproxy","text":"","title":"mitmproxy"},{"location":"cheatsheet/#methods-of-adding-proxy-certificate","text":"","title":"Methods of adding proxy certificate"},{"location":"cheatsheet/#ansible_1","text":"","title":"Ansible"},{"location":"cheatsheet/#centos","text":"- hosts: rizk_dev become: yes vars: local_cert_path: ./certs/ rizk_dev_cert_path: /etc/pki/ca-trust/source/anchors/ handlers: - name: restart php-fpm service: name: php-fpm state: restarted tasks: - name: install ca package on rhel systems yum: name: ca-certificates state: present - name: enable dynamic ca configuration on rhel6 shell: /bin/update-ca-trust enable - name: push mitmproxy certificates synchronize: src: \"{{ local_cert_path }}\" dest: \"{{ rizk_dev_cert_path }}\" - name: file: path: \"{{ rizk_dev_cert_path }}\" state: directory recurse: true owner: root group: root mode: 0644 - name: update trusted ca redhat shell: /bin/update-ca-trust register: update_ca_trust - name: Add rizk-dev to hosts file lineinfile: path: \"/home/{{ ansible_user }}/.bashrc\" regexp: \"{{ item.regexp }}\" line: \"{{ item.line }}\" become_user: \"{{ ansible_user }}\" with_items: - regexp: \"^alias\\\\sset-default-gateway\" line: \"alias set-default-gateway='sudo route del default gw 192.168.29.100 && sudo route add default gw 10.0.2.2'\" - regexp: \"^alias\\\\sset-mitmproxy-gateway\" line: \"alias set-mitmproxy-gateway='sudo route del default gw 10.0.2.2 && sudo route add default gw 192.168.29.100'\" - regexp: \"^alias\\\\srestart-docker\" line: \"alias restart-dockr='sudo systemctl restart docker'\" - name: Enable proxy for php-fpm lineinfile: path: \"/etc/php-fpm.d/www.conf\" regexp: \"{{ item.regexp }}\" line: \"{{ item.line }}\" with_items: - regexp: \"^env\\\\[HTTP_PROXY\\\\]\" line: \"env[HTTP_PROXY] = 192.168.29.100:8080\" - regexp: \"^env\\\\[HTTPS_PROXY\\\\]\" line: \"env[HTTPS_PROXY] = 192.168.29.100:8080\" tags: [never, enable-proxy] notify: restart php-fpm - name: Disbale proxy for php-fpm lineinfile: path: \"/etc/php-fpm.d/www.conf\" regexp: \"{{ item }}\" state: absent loop: - \"^env\\\\[HTTP_PROXY\\\\]\" - \"^env\\\\[HTTPS_PROXY\\\\]\" tags: [never, disable-proxy] notify: restart php-fpm","title":"CentOS"},{"location":"cheatsheet/#ubuntu","text":"- name: push mitmproxy certificates synchronize: src: mitmproxy-certs/mitmproxy-ca-cert.pem dest: /usr/local/share/ca-certificates/mitmproxy-ca-cert.crt - name: file: path: /usr/local/share/ca-certificates state: directory recurse: true owner: root group: root mode: 0644 - name: Update trusted ca shell: /usr/sbin/update-ca-certificates register: update_ca - debug: msg: \"{{ update_ca }}\"","title":"Ubuntu"},{"location":"cheatsheet/#docker-alpine","text":"FROM node:12.13.1-alpine3.10 AS builder COPY ./package.json /app/package.json COPY ./.env.development.local /app/.env.development.local COPY ./backend /app/backend WORKDIR /app RUN [\"apk\", \"add\", \"--update\", \"nodejs\", \"npm\"] RUN [\"npm\", \"install\"] FROM node:12.13.1-alpine3.10 WORKDIR /app COPY --from=builder /app /app RUN apk update && apk add ca-certificates && rm -rf /var/cache/apk/* COPY ./mitmproxy-ca-cert.crt /usr/local/share/ca-certificates/mitmproxy-ca-cert.crt RUN update-ca-certificates CMD [\"npm\", \"run\", \"server:dev\"] version: \"3.6\" services: backend: build: context: ./ dockerfile: Dev.Backend.Dockerfile ports: - 8181:8181 volumes: - ./backend:/app/backend environment: - NODE_TLS_REJECT_UNAUTHORIZED=0 - HTTP_PROXY=http://192.168.29.200:8080 - HTTPS_PROXY=http://192.168.29.200:8080 frontend: build: context: ./ dockerfile: Dev.Frontend.Dockerfile ports: - 3000:3000 volumes: - ./frontend:/app/frontend","title":"Docker Alpine"},{"location":"cheatsheet/#heroku","text":"","title":"heroku"},{"location":"cheatsheet/#commands_6","text":"heroku -v heroku apps --team=betsson-zecure heroku builds -a source-of-wealth-staging heroku config --app=source-of-wealth-staging heroku container:login heroku container:push web --app=source-of-wealth-staging heroku container:release web --app=source-of-wealth-staging heroku login heroku logs --app=source-of-wealth-staging heroku plugins:install heroku-builds heroku ps -a source-of-wealth-staging heroku ps:kill release.7483 -a source-of-wealth-staging","title":"commands"},{"location":"cheatsheet/#docker","text":"","title":"docker"},{"location":"cheatsheet/#commands_7","text":"docker build -t source-of-wealth:latest . docker run --name source-of-wealth -d -p 3000:3000 --env-file ./.env source-of-wealth docker container stop source-of-wealth docker container rm source-of-wealth docker exec -it source-of-wealth /bin/sh","title":"commands"},{"location":"cheatsheet/#ssh","text":"# generate ssh private / public key ssh-keygen -t rsa -b 2048 -C \"email@example.com\" # copy public key to clipboard xclip -sel clip < ~/.ssh/id_rsa.pub # test wether ssh key added correctly to github ssh -T git@github.com","title":"ssh"},{"location":"iptables/","text":"IPTables Source Introduction iptables places rules into predefined chains (INPUT, OUTPUT and FORWARD) rules are checked against any network traffic (IP packets) relevant to these chains a decision is made about what to do with each packet based upon the outcome of those rules actions are referred to as targets , of which two most common are DROP or ACCEPT Chains INPUT - all packets destined for host computer OUTPUT - all packets originating frome host computer FORWARD - All packets neither destined for nor originating from the host computer but passing through the host computer. This chain is used if you are using your computer as a router rules ared added in a list to each chain a packet is checked against each rule in turn, starting at the top if it matches taht rule, an action is taken such as accepting or dropping the packet once a rule has been matched and an action taken, the packet is processed according to the outcome of the rule and isn't processed by any further rules down the chain if a packet passes through all the rules in the chain and reaches the bottom without being matched against any rule, then the default action for that chain is taken this is referred to as the default policy and may be set to either ACCEPT or DROP this concept of default policies with chains raises two strategies of how to organize our firewall 1st strategy is to set default policy to DROP and add rules to allow packets from trusted IPs, or certain ports on which services are running 2nd strategy is to set default policy to ACCEPT and then add rules that block packets form IP addresses or ranges, or for certain ports on which we have private services or no services running generally the 1st strategy is used for INPUT chanin where we want to control what is allowed to access our machine the 2nd strategy is used for OUTPUT chain where we generally trust traffic that is leaving our machine Getting started","title":"Iptables"},{"location":"iptables/#iptables","text":"Source","title":"IPTables"},{"location":"iptables/#introduction","text":"iptables places rules into predefined chains (INPUT, OUTPUT and FORWARD) rules are checked against any network traffic (IP packets) relevant to these chains a decision is made about what to do with each packet based upon the outcome of those rules actions are referred to as targets , of which two most common are DROP or ACCEPT","title":"Introduction"},{"location":"iptables/#chains","text":"INPUT - all packets destined for host computer OUTPUT - all packets originating frome host computer FORWARD - All packets neither destined for nor originating from the host computer but passing through the host computer. This chain is used if you are using your computer as a router rules ared added in a list to each chain a packet is checked against each rule in turn, starting at the top if it matches taht rule, an action is taken such as accepting or dropping the packet once a rule has been matched and an action taken, the packet is processed according to the outcome of the rule and isn't processed by any further rules down the chain if a packet passes through all the rules in the chain and reaches the bottom without being matched against any rule, then the default action for that chain is taken this is referred to as the default policy and may be set to either ACCEPT or DROP this concept of default policies with chains raises two strategies of how to organize our firewall 1st strategy is to set default policy to DROP and add rules to allow packets from trusted IPs, or certain ports on which services are running 2nd strategy is to set default policy to ACCEPT and then add rules that block packets form IP addresses or ranges, or for certain ports on which we have private services or no services running generally the 1st strategy is used for INPUT chanin where we want to control what is allowed to access our machine the 2nd strategy is used for OUTPUT chain where we generally trust traffic that is leaving our machine","title":"Chains"},{"location":"iptables/#getting-started","text":"","title":"Getting started"},{"location":"understanding_typescript/","text":"Understanding Typescript Getting Started Installing and using using typescript You can install typescript interpreter using npm npm install -g typescript To use the interpreter you need a typescript file // using-ts.ts const button = document.querySelector(\"button\"); const input1 = document.getElementById(\"num1\")! as HTMLInputElement; const input2 = document.getElementById(\"num2\")! as HTMLInputElement; function add(num1: number, num2: number) { return num1 + num2; } button.addEventListener(\"click\", function() { console.log(add(+input1.value, +input2.value)); }); Run the following command tsc using-ts.ts And you will get the following output // using-ts.js var button = document.querySelector(\"button\"); var input1 = document.getElementById(\"num1\"); var input2 = document.getElementById(\"num2\"); function add(num1, num2) { return num1 + num2; } button.addEventListener(\"click\", function () { console.log(add(+input1.value, +input2.value)); }); TypesScript Basics and Basic Types Using Types Core Types: number 1, 5.3, -10 string 'Hi', \"Hi\", `Hi` boolean true, false object: {age: 30} Array: [1, 2, 3] Tuple: [1, 2] Enum: enum{ NEW, OLD} Example: function add(n1: number, n2: number) { return n1 + n2; } const number1 = 5; const number2 = 2.8; const result = add(number1, number2); console.log(result); Important: Type Casing - The core primitive types in TypeScript are all lowercase! Working with Numbers, Strings and Booleans Example: function add(n1: number, n2: number, showResult: boolean, phrase: string) { // if (typeof n1 !== 'number' || typeof n2 !== 'number') { // throw new Error('Incorrect input!'); // } const result = n1 + n2; if (showResult) { console.log(phrase + result); } else { return result; } } const number1 = 5; // 5.0 const number2 = 2.8; const printResult = true; const resultPhrase = 'Result is: '; add(number1, number2, printResult, resultPhrase); Type Assignment & Type Inference when a variable has been assigned a value at declaration, you do not need to assign a type as typescript works out the type by inference. if you declare a variable without assigning a value, you need to declare the variable type let number1: number; number1 = 5; Working with Tuples const person: { name: string; age: number; hobbies: string[]; role: [number, string]; } = { name: 'Maximilian', age: 30, hobbies: ['Sports', 'Cooking'], role: [2, 'author'] }; Working with Enums enum Role { ADMIN = 'ADMIN', READ_ONLY = 100, AUTHOR = 'AUTHOR' }; const person = { name: 'Maximilian', age: 30, hobbies: ['Sports', 'Cooking'], role: Role.ADMIN }; Working with Union Types function combine(input1: number | string, input2: number | string) { let result; if (typeof input1 === 'number' && typeof input2 === 'number') { result = input1 + input2; } else { result = input1.toString() + input2.toString(); } return result; } const combinedAges = combine(30, 26); console.log(combinedAges); const combinedNames = combine('Max', 'Anna'); console.log(combinedNames); Working with Literal Types function combine( input1: number | string, input2: number | string, resultConversion: 'as-number' | 'as-text' ) { let result; if (typeof input1 === 'number' && typeof input2 === 'number' || resultConversion === 'as-number') { result = +input1 + +input2; } else { result = input1.toString() + input2.toString(); } return result; } const combinedAges = combine(30, 26, 'as-number'); console.log(combinedAges); const combinedStringAges = combine('30', '26', 'as-number'); console.log(combinedStringAges); const combinedNames = combine('Max', 'Anna', 'as-text'); console.log(combinedNames); Type Aliases / Custom Types type Combinable = number | string; type ConversionDescriptor = 'as-number' | 'as-text'; function combine( input1: Combinable, input2: Combinable, resultConversion: ConversionDescriptor ) { let result; if (typeof input1 === 'number' && typeof input2 === 'number' || resultConversion === 'as-number') { result = +input1 + +input2; } else { result = input1.toString() + input2.toString(); } return result; } const combinedAges = combine(30, 26, 'as-number'); console.log(combinedAges); const combinedStringAges = combine('30', '26', 'as-number'); console.log(combinedStringAges); const combinedNames = combine('Max', 'Anna', 'as-text'); console.log(combinedNames); Type aliases can be used to \"create\" your own types. You're not limited to storing union types though - you can also provide an alias to a (possibly complex) object type. For example: type User = { name: string; age: number }; const u1: User = { name: 'Max', age: 30 }; // this works! This allows you to avoid unnecessary repetition and manage types centrally. For example, you can simplify this code: function greet(user: { name: string; age: number }) { console.log('Hi, I am ' + user.name); } function isOlder(user: { name: string; age: number }, checkAge: number) { return checkAge > user.age; } To: type User = { name: string; age: number }; function greet(user: User) { console.log('Hi, I am ' + user.name); } function isOlder(user: User, checkAge: number) { return checkAge > user.age; } Function Return Types & \"void\" function add(n1: number, n2: number): number { return n1 + n2; } function printResult(num: number): void { console.log('Result: ' + num); } function printResult2(num: number): undefined { console.log('Result: ' + num); return; } Functions as Types function add(n1: number, n2: number): number { return n1 + n2; } let combineValues: Function; let combineValuesTypes: (a: number, b: number) => number; combineValues = add; combineValuesTypes = add; Function Types & Callbacks function addAndHandle(n1: number, n2: number, cb: (num: number) => void) { const result = n1 + n2; cb(result); } addAndHandle(10, 20, (result) => { console.log(result) }); The \"unknown\" type let userInput: unknown; let userName: string; userInput = 5; userInput = 'Max'; if (typeof userInput === 'string') { userName = userInput; } The \"never\" Type function generateError(message: string, code: number): never { throw { message: message, errorCode: code }; // while (true) {} } generateError('An error occurred!', 500); The TypeScript Compiler (and its configuration) Using \"Watch Mode\" tsc app.ts -w tsc app.ts --watch Compiling the Entire Project / Multiple Files tsc --init tsc tsc --watch Understanding Typescript Core Libs tsconfig docs compiler options docs Next-Generation JavaScript & TypeScript \"let\" and \"const\" // const value cannot be changed after assignment const userName = \"Max\"; let age = 30; age = 29; function add(a: number, b: number) { let result; result = a + b; return result; } if (age > 20) { // isOld is available outside of block scope var isOld = true; } if (age > 30) { // isOlder is available only within block scope let isOlder = true; } Arrow functions const add = (a: number, b: number) => { return a + b; }; const addAlt = (a: number, b: number) => a + b; const printOutput: (a: number | string) => void = (output) => console.log(output); const button = document.querySelector(\"button\"); if (button) { button.addEventListener(\"click\", (event) => console.log(event)); } printOutput(add(2, 5)); Default Function Parameters const add = (a: number, b: number = 1) => a + b; console.log(add(4)); The Spread Operator (...) const hobbies = [\"Sports\", \"Cooking\"]; // Method 1 const activeHobbies = [\"Hiking\", ...hobbies]; // Method 2 activeHobbies.push(...hobbies); const person = { name: \"Max\", age: 30, }; const copiedPerson = { ...person }; Rest Parameters const add = (...numbers: number[]) => { return numbers.reduce((curResult, curValue) => { return curResult + curValue; }, 0); }; const addWithTuples = (...numbers: [number, number, number]) => { return numbers.reduce((curResult, curValue) => { return curResult + curValue; }, 0); }; const addedNumbers = add(5, 10, 2, 3.7); const addedNumbersTuples = addWithTuples(1, 2, 3) Array & Object Destructuring // Array Destructuring const hobbies = [\"Sports\", \"Cooking\", \"Hiking\", \"Gardening\"]; const [hobby1, hobby2, ...remainingHobbies] = hobbies; // Object Destructuring const person = { firstName: \"Max\", age: 30, }; const { firstName: userName, age } = person; console.log(userName, age); Classes and Interfaces Creating a First Class class Department { name: string = \"DEFAULT\"; constructor(n: string) { this.name = n; } } const accounting = new Department(\"Accounting\"); console.log(accounting); Constructor Functions & The \"this\" Keyword class Department { name: string = \"DEFAULT\"; constructor(n: string) { this.name = n; } // Optional: add extra type safety describe(this: Department) { console.log(\"Department: \" + this.name); } } const accounting = new Department(\"Accounting\"); accounting.describe(); const accountingCopy = { name: \"DUMMY\", describe: accounting.describe }; accountingCopy.describe(); // this typically refers to the thing which is responsible for calling the method \"private\" and \"private\" Access Modifiers class Department { // NOTE: public modifier is default, no need to write public name: string; private employees: string[] = []; constructor(n: string) { this.name = n; } describe(this: Department) { console.log(\"Department: \" + this.name); } addEmployee(employee: string) { this.employees.push(employee); } printEmployeeInformation() { console.log(this.employees.length); console.log(this.employees); } } const accounting = new Department(\"Accounting\"); accounting.addEmployee('Max'); accounting.addEmployee('Manu'); accounting.describe() accounting.printEmployeeInformation(); Shorthand Initialization class Department { constructor(private id: string, public name: string) {} describe(this: Department) { console.log(`Department (${this.id}): ` + this.name); } } const accounting = new Department(\"acn\", \"Accounting\"); accounting.describe(); \"readonly\" Properties class Department { // readonly does not allow reassignment of property constructor(private readonly id: string, public name: string) {} describe(this: Department) { console.log(`Department (${this.id}): ` + this.name); } } const accounting = new Department(\"acn\", \"Accounting\"); accounting.describe(); Inheritance class Department { private employees: string[] = []; constructor(private readonly id: string, public name: string) {} describe(this: Department) { console.log(`Deparment (${this.id}): ${this.name}`); } addEmployee(employee: string) { this.employees.push(employee); } printEmployeeInformation() { console.log(this.employees.length); console.log(this.employees); } } class ITDeparment extends Department { constructor(id: string, public admins: string[]) { super(id, \"IT\"); } } const it = new ITDeparment(\"IT1\", [\"John\", \"Micheal\"]); it.addEmployee(\"Amy\"); it.addEmployee(\"Dory\"); it.describe(); it.printEmployeeInformation(); console.log(it); Overriding Properites & The \"protected\" Modifier class Department { // private properties cannot be inherited, // if you want them to be inherited they // need to be changed to protected protected employees: string[] = []; constructor(private readonly id: string, public name: string) {} addEmployee(employee: string) { this.employees.push(employee); } describe(this: Department) { console.log(`Deparment (${this.id}): ${this.name}`); } printEmployeeInformation() { console.log(this.employees.length); console.log(this.employees); } } class AccountingDepartment extends Department { constructor(id: string) { super(id, \"Accounting\"); } addEmployee(name: string) { if (name === \"Max\") { return; } this.employees.push(name); } } const accounting = new AccountingDepartment(\"d2\"); accounting.addEmployee(\"Max\"); accounting.addEmployee(\"Manu\"); accounting.printEmployeeInformation(); Getters & Setters class Department { protected employees: string[] = []; constructor(private readonly id: string, public name: string) {} describe(this: Department) { console.log(`Department (${this.id}): ${this.name}`); } addEmployee(employee: string) { this.employees.push(employee); } printEmployeeInformation() { console.log(this.employees.length); console.log(this.employees); } } class AccountingDepartment extends Department { private lastReport: string; get mostRecentReport() { if (this.lastReport) { return this.lastReport; } throw new Error(\"No report found.\"); } set mostRecentReport(value: string) { if (!value) { throw new Error(\"Please pass in a valid value!\"); } this.addReport(value); } constructor(id: string, private reports: string[]) { super(id, \"Accounting\"); this.lastReport = reports[0]; } addEmployee(name: string) { if (name === \"Max\") { return; } this.employees.push(name); } addReport(text: string) { this.reports.push(text); this.lastReport = text; } printReports() { console.log(this.reports); } } const accounting = new AccountingDepartment(\"d2\", []); accounting.mostRecentReport = \"Year End Report\"; accounting.addReport(\"Something went wrong...\"); console.log(accounting.mostRecentReport); accounting.addEmployee(\"Max\"); accounting.addEmployee(\"Manu\"); accounting.printReports(); accounting.printEmployeeInformation();","title":"Understanding Typescript"},{"location":"understanding_typescript/#understanding-typescript","text":"","title":"Understanding Typescript"},{"location":"understanding_typescript/#getting-started","text":"","title":"Getting Started"},{"location":"understanding_typescript/#installing-and-using-using-typescript","text":"You can install typescript interpreter using npm npm install -g typescript To use the interpreter you need a typescript file // using-ts.ts const button = document.querySelector(\"button\"); const input1 = document.getElementById(\"num1\")! as HTMLInputElement; const input2 = document.getElementById(\"num2\")! as HTMLInputElement; function add(num1: number, num2: number) { return num1 + num2; } button.addEventListener(\"click\", function() { console.log(add(+input1.value, +input2.value)); }); Run the following command tsc using-ts.ts And you will get the following output // using-ts.js var button = document.querySelector(\"button\"); var input1 = document.getElementById(\"num1\"); var input2 = document.getElementById(\"num2\"); function add(num1, num2) { return num1 + num2; } button.addEventListener(\"click\", function () { console.log(add(+input1.value, +input2.value)); });","title":"Installing and using using typescript"},{"location":"understanding_typescript/#typesscript-basics-and-basic-types","text":"","title":"TypesScript Basics and Basic Types"},{"location":"understanding_typescript/#using-types","text":"Core Types: number 1, 5.3, -10 string 'Hi', \"Hi\", `Hi` boolean true, false object: {age: 30} Array: [1, 2, 3] Tuple: [1, 2] Enum: enum{ NEW, OLD} Example: function add(n1: number, n2: number) { return n1 + n2; } const number1 = 5; const number2 = 2.8; const result = add(number1, number2); console.log(result); Important: Type Casing - The core primitive types in TypeScript are all lowercase!","title":"Using Types"},{"location":"understanding_typescript/#working-with-numbers-strings-and-booleans","text":"Example: function add(n1: number, n2: number, showResult: boolean, phrase: string) { // if (typeof n1 !== 'number' || typeof n2 !== 'number') { // throw new Error('Incorrect input!'); // } const result = n1 + n2; if (showResult) { console.log(phrase + result); } else { return result; } } const number1 = 5; // 5.0 const number2 = 2.8; const printResult = true; const resultPhrase = 'Result is: '; add(number1, number2, printResult, resultPhrase);","title":"Working with Numbers, Strings and Booleans"},{"location":"understanding_typescript/#type-assignment-type-inference","text":"when a variable has been assigned a value at declaration, you do not need to assign a type as typescript works out the type by inference. if you declare a variable without assigning a value, you need to declare the variable type let number1: number; number1 = 5;","title":"Type Assignment &amp; Type Inference"},{"location":"understanding_typescript/#working-with-tuples","text":"const person: { name: string; age: number; hobbies: string[]; role: [number, string]; } = { name: 'Maximilian', age: 30, hobbies: ['Sports', 'Cooking'], role: [2, 'author'] };","title":"Working with Tuples"},{"location":"understanding_typescript/#working-with-enums","text":"enum Role { ADMIN = 'ADMIN', READ_ONLY = 100, AUTHOR = 'AUTHOR' }; const person = { name: 'Maximilian', age: 30, hobbies: ['Sports', 'Cooking'], role: Role.ADMIN };","title":"Working with Enums"},{"location":"understanding_typescript/#working-with-union-types","text":"function combine(input1: number | string, input2: number | string) { let result; if (typeof input1 === 'number' && typeof input2 === 'number') { result = input1 + input2; } else { result = input1.toString() + input2.toString(); } return result; } const combinedAges = combine(30, 26); console.log(combinedAges); const combinedNames = combine('Max', 'Anna'); console.log(combinedNames);","title":"Working with Union Types"},{"location":"understanding_typescript/#working-with-literal-types","text":"function combine( input1: number | string, input2: number | string, resultConversion: 'as-number' | 'as-text' ) { let result; if (typeof input1 === 'number' && typeof input2 === 'number' || resultConversion === 'as-number') { result = +input1 + +input2; } else { result = input1.toString() + input2.toString(); } return result; } const combinedAges = combine(30, 26, 'as-number'); console.log(combinedAges); const combinedStringAges = combine('30', '26', 'as-number'); console.log(combinedStringAges); const combinedNames = combine('Max', 'Anna', 'as-text'); console.log(combinedNames);","title":"Working with Literal Types"},{"location":"understanding_typescript/#type-aliases-custom-types","text":"type Combinable = number | string; type ConversionDescriptor = 'as-number' | 'as-text'; function combine( input1: Combinable, input2: Combinable, resultConversion: ConversionDescriptor ) { let result; if (typeof input1 === 'number' && typeof input2 === 'number' || resultConversion === 'as-number') { result = +input1 + +input2; } else { result = input1.toString() + input2.toString(); } return result; } const combinedAges = combine(30, 26, 'as-number'); console.log(combinedAges); const combinedStringAges = combine('30', '26', 'as-number'); console.log(combinedStringAges); const combinedNames = combine('Max', 'Anna', 'as-text'); console.log(combinedNames); Type aliases can be used to \"create\" your own types. You're not limited to storing union types though - you can also provide an alias to a (possibly complex) object type. For example: type User = { name: string; age: number }; const u1: User = { name: 'Max', age: 30 }; // this works! This allows you to avoid unnecessary repetition and manage types centrally. For example, you can simplify this code: function greet(user: { name: string; age: number }) { console.log('Hi, I am ' + user.name); } function isOlder(user: { name: string; age: number }, checkAge: number) { return checkAge > user.age; } To: type User = { name: string; age: number }; function greet(user: User) { console.log('Hi, I am ' + user.name); } function isOlder(user: User, checkAge: number) { return checkAge > user.age; }","title":"Type Aliases / Custom Types"},{"location":"understanding_typescript/#function-return-types-void","text":"function add(n1: number, n2: number): number { return n1 + n2; } function printResult(num: number): void { console.log('Result: ' + num); } function printResult2(num: number): undefined { console.log('Result: ' + num); return; }","title":"Function Return Types &amp; \"void\""},{"location":"understanding_typescript/#functions-as-types","text":"function add(n1: number, n2: number): number { return n1 + n2; } let combineValues: Function; let combineValuesTypes: (a: number, b: number) => number; combineValues = add; combineValuesTypes = add;","title":"Functions as Types"},{"location":"understanding_typescript/#function-types-callbacks","text":"function addAndHandle(n1: number, n2: number, cb: (num: number) => void) { const result = n1 + n2; cb(result); } addAndHandle(10, 20, (result) => { console.log(result) });","title":"Function Types &amp; Callbacks"},{"location":"understanding_typescript/#the-unknown-type","text":"let userInput: unknown; let userName: string; userInput = 5; userInput = 'Max'; if (typeof userInput === 'string') { userName = userInput; }","title":"The \"unknown\" type"},{"location":"understanding_typescript/#the-never-type","text":"function generateError(message: string, code: number): never { throw { message: message, errorCode: code }; // while (true) {} } generateError('An error occurred!', 500);","title":"The \"never\" Type"},{"location":"understanding_typescript/#the-typescript-compiler-and-its-configuration","text":"","title":"The TypeScript Compiler (and its configuration)"},{"location":"understanding_typescript/#using-watch-mode","text":"tsc app.ts -w tsc app.ts --watch","title":"Using \"Watch Mode\""},{"location":"understanding_typescript/#compiling-the-entire-project-multiple-files","text":"tsc --init tsc tsc --watch","title":"Compiling the Entire Project / Multiple Files"},{"location":"understanding_typescript/#understanding-typescript-core-libs","text":"tsconfig docs compiler options docs","title":"Understanding Typescript Core Libs"},{"location":"understanding_typescript/#next-generation-javascript-typescript","text":"","title":"Next-Generation JavaScript &amp; TypeScript"},{"location":"understanding_typescript/#let-and-const","text":"// const value cannot be changed after assignment const userName = \"Max\"; let age = 30; age = 29; function add(a: number, b: number) { let result; result = a + b; return result; } if (age > 20) { // isOld is available outside of block scope var isOld = true; } if (age > 30) { // isOlder is available only within block scope let isOlder = true; }","title":"\"let\" and \"const\""},{"location":"understanding_typescript/#arrow-functions","text":"const add = (a: number, b: number) => { return a + b; }; const addAlt = (a: number, b: number) => a + b; const printOutput: (a: number | string) => void = (output) => console.log(output); const button = document.querySelector(\"button\"); if (button) { button.addEventListener(\"click\", (event) => console.log(event)); } printOutput(add(2, 5));","title":"Arrow functions"},{"location":"understanding_typescript/#default-function-parameters","text":"const add = (a: number, b: number = 1) => a + b; console.log(add(4));","title":"Default Function Parameters"},{"location":"understanding_typescript/#the-spread-operator","text":"const hobbies = [\"Sports\", \"Cooking\"]; // Method 1 const activeHobbies = [\"Hiking\", ...hobbies]; // Method 2 activeHobbies.push(...hobbies); const person = { name: \"Max\", age: 30, }; const copiedPerson = { ...person };","title":"The Spread Operator (...)"},{"location":"understanding_typescript/#rest-parameters","text":"const add = (...numbers: number[]) => { return numbers.reduce((curResult, curValue) => { return curResult + curValue; }, 0); }; const addWithTuples = (...numbers: [number, number, number]) => { return numbers.reduce((curResult, curValue) => { return curResult + curValue; }, 0); }; const addedNumbers = add(5, 10, 2, 3.7); const addedNumbersTuples = addWithTuples(1, 2, 3)","title":"Rest Parameters"},{"location":"understanding_typescript/#array-object-destructuring","text":"// Array Destructuring const hobbies = [\"Sports\", \"Cooking\", \"Hiking\", \"Gardening\"]; const [hobby1, hobby2, ...remainingHobbies] = hobbies; // Object Destructuring const person = { firstName: \"Max\", age: 30, }; const { firstName: userName, age } = person; console.log(userName, age);","title":"Array &amp; Object Destructuring"},{"location":"understanding_typescript/#classes-and-interfaces","text":"","title":"Classes and Interfaces"},{"location":"understanding_typescript/#creating-a-first-class","text":"class Department { name: string = \"DEFAULT\"; constructor(n: string) { this.name = n; } } const accounting = new Department(\"Accounting\"); console.log(accounting);","title":"Creating a First Class"},{"location":"understanding_typescript/#constructor-functions-the-this-keyword","text":"class Department { name: string = \"DEFAULT\"; constructor(n: string) { this.name = n; } // Optional: add extra type safety describe(this: Department) { console.log(\"Department: \" + this.name); } } const accounting = new Department(\"Accounting\"); accounting.describe(); const accountingCopy = { name: \"DUMMY\", describe: accounting.describe }; accountingCopy.describe(); // this typically refers to the thing which is responsible for calling the method","title":"Constructor Functions &amp; The \"this\" Keyword"},{"location":"understanding_typescript/#private-and-private-access-modifiers","text":"class Department { // NOTE: public modifier is default, no need to write public name: string; private employees: string[] = []; constructor(n: string) { this.name = n; } describe(this: Department) { console.log(\"Department: \" + this.name); } addEmployee(employee: string) { this.employees.push(employee); } printEmployeeInformation() { console.log(this.employees.length); console.log(this.employees); } } const accounting = new Department(\"Accounting\"); accounting.addEmployee('Max'); accounting.addEmployee('Manu'); accounting.describe() accounting.printEmployeeInformation();","title":"\"private\" and \"private\" Access Modifiers"},{"location":"understanding_typescript/#shorthand-initialization","text":"class Department { constructor(private id: string, public name: string) {} describe(this: Department) { console.log(`Department (${this.id}): ` + this.name); } } const accounting = new Department(\"acn\", \"Accounting\"); accounting.describe();","title":"Shorthand Initialization"},{"location":"understanding_typescript/#readonly-properties","text":"class Department { // readonly does not allow reassignment of property constructor(private readonly id: string, public name: string) {} describe(this: Department) { console.log(`Department (${this.id}): ` + this.name); } } const accounting = new Department(\"acn\", \"Accounting\"); accounting.describe();","title":"\"readonly\" Properties"},{"location":"understanding_typescript/#inheritance","text":"class Department { private employees: string[] = []; constructor(private readonly id: string, public name: string) {} describe(this: Department) { console.log(`Deparment (${this.id}): ${this.name}`); } addEmployee(employee: string) { this.employees.push(employee); } printEmployeeInformation() { console.log(this.employees.length); console.log(this.employees); } } class ITDeparment extends Department { constructor(id: string, public admins: string[]) { super(id, \"IT\"); } } const it = new ITDeparment(\"IT1\", [\"John\", \"Micheal\"]); it.addEmployee(\"Amy\"); it.addEmployee(\"Dory\"); it.describe(); it.printEmployeeInformation(); console.log(it);","title":"Inheritance"},{"location":"understanding_typescript/#overriding-properites-the-protected-modifier","text":"class Department { // private properties cannot be inherited, // if you want them to be inherited they // need to be changed to protected protected employees: string[] = []; constructor(private readonly id: string, public name: string) {} addEmployee(employee: string) { this.employees.push(employee); } describe(this: Department) { console.log(`Deparment (${this.id}): ${this.name}`); } printEmployeeInformation() { console.log(this.employees.length); console.log(this.employees); } } class AccountingDepartment extends Department { constructor(id: string) { super(id, \"Accounting\"); } addEmployee(name: string) { if (name === \"Max\") { return; } this.employees.push(name); } } const accounting = new AccountingDepartment(\"d2\"); accounting.addEmployee(\"Max\"); accounting.addEmployee(\"Manu\"); accounting.printEmployeeInformation();","title":"Overriding Properites &amp; The \"protected\" Modifier"},{"location":"understanding_typescript/#getters-setters","text":"class Department { protected employees: string[] = []; constructor(private readonly id: string, public name: string) {} describe(this: Department) { console.log(`Department (${this.id}): ${this.name}`); } addEmployee(employee: string) { this.employees.push(employee); } printEmployeeInformation() { console.log(this.employees.length); console.log(this.employees); } } class AccountingDepartment extends Department { private lastReport: string; get mostRecentReport() { if (this.lastReport) { return this.lastReport; } throw new Error(\"No report found.\"); } set mostRecentReport(value: string) { if (!value) { throw new Error(\"Please pass in a valid value!\"); } this.addReport(value); } constructor(id: string, private reports: string[]) { super(id, \"Accounting\"); this.lastReport = reports[0]; } addEmployee(name: string) { if (name === \"Max\") { return; } this.employees.push(name); } addReport(text: string) { this.reports.push(text); this.lastReport = text; } printReports() { console.log(this.reports); } } const accounting = new AccountingDepartment(\"d2\", []); accounting.mostRecentReport = \"Year End Report\"; accounting.addReport(\"Something went wrong...\"); console.log(accounting.mostRecentReport); accounting.addEmployee(\"Max\"); accounting.addEmployee(\"Manu\"); accounting.printReports(); accounting.printEmployeeInformation();","title":"Getters &amp; Setters"},{"location":"vagrant_recipes/","text":"Vagrant Recipes # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\"2\") do |config| config.vm.box = \"bento/debian-10.2\" config.vm.network \"public_network\", ip: \"192.168.0.130\" config.vm.provider :virtualbox do |v| v.memory = 512 v.cpus = 1 v.name = \"vm_debian\" end end # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\"2\") do |config| config.vm.box = \"bento/ubuntu-19.10\" config.vm.network \"public_network\", ip: \"192.168.0.131\" config.vm.provider :virtualbox do |v| v.memory = 512 v.cpus = 1 v.name = \"vm_ubuntu\" end end","title":"Vagrant Recipes"},{"location":"vagrant_recipes/#vagrant-recipes","text":"# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\"2\") do |config| config.vm.box = \"bento/debian-10.2\" config.vm.network \"public_network\", ip: \"192.168.0.130\" config.vm.provider :virtualbox do |v| v.memory = 512 v.cpus = 1 v.name = \"vm_debian\" end end # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\"2\") do |config| config.vm.box = \"bento/ubuntu-19.10\" config.vm.network \"public_network\", ip: \"192.168.0.131\" config.vm.provider :virtualbox do |v| v.memory = 512 v.cpus = 1 v.name = \"vm_ubuntu\" end end","title":"Vagrant Recipes"},{"location":"vagrant_tutorial/","text":"Vagrant tutorial source Install Install vagrant via binary package always link sudo apt update curl -O https://releases.hashicorp.com/vagrant/2.2.7/vagrant_2.2.7_x86_64.deb sudo apt install ./vagrant_2.2.7_x86_64.deb vagrant --version Project Setup mkdir <root-project-dir> cd <root-project-dir> vagrant init hashicorp/bionic64 Boxes vagrant uses base images to quickly clone a virtual machine base images are known as \"boxes\" boxes are added to vagrant with vagrant box add vagrant box add hashicorp/bionic64 this will download box named hashicorp/bionic64 boxes are globally stored for current user each project uses box as initial image to clone from, but nevet modifies actual base Using a Box open VagrantFile and change the contents to the following Vagrant.configure(\"2\") do |config| config.vm.box = \"hashicorp/bionic64\" # optionally specify explicit version of box config.vm.box_version = \"1.1.0\" # optionally specify url to box directly config.vm.box_url = \"https://vagrantcloud.com/hashicorp/bionc64\" end best place to find more boxes is HashiCorp's Vagrant Cloud box catalog Up and SSH # boot vagrant environment vagrant up # ssh into vm vagrant ssh # SSH session can be terminated via <C-d> # destroy vm vagrant destroy # remove downloaded box vagrant box remove be careful not to rm -rf / since vagrant shares a directory at /vagrant with directory on host and this can delete all files in shared folder Synced Folders by default vagrant shares project directory to /vagrant folder Provisioning vagrant has built in support for automated provisioning vagrant will automatically install software when you vagrant up Installing apache # bootstap.sh #!/usr/bin/env bash apt-get update apt-get install -y apache2 if ! [ -L /var/www ]; then rm -rf /var/www ln -fs /vagrant /var/www fi Vagrant.configure(\"2\") do |config| config.vm.box = \"hashicorp/bionic64\" config.vm.provision :shell, path: \"bootstrap.sh\" end create machine with vagrant up and vagrant automatically will provision if guest machine already running, run vagrant reload --provision Networking Port Forwarding Port forwarding allows to specify ports on guest machine to share via a port on host machine Vagrant.configure(\"2\") do |config| config.vm.box = \"hashicorp/bionic64\" config.vm.provision :shell, path: \"bootstrap.sh\" config.vm.network :forwarded_port, guest: 80, host: 2567 end See more Share vagrant shares let you share your environment to anyone who has an internet connection will generate a URL that will route directly to your environment to install run the following command vagrant plugin install vagrant-share next run the following command vagrant share Teardown with vagrant you can suspend , halt , or destroy Suspending vagrant suspend will save current running state of machine and stop it. resume work by calling vagrant up it is super fast but eats up disk space and requires more to store state of RAM on disk Halting vagrant halt will gracefully shut down guest OS and power down guest machine user vagrant up to boot up again cleanly shuts down machine, preserving contents on disk Destorying vagrant destroy removes all traces of guest machine Providers Vagrant can work with a variety of backend providers such as VMware and Hyper-V Once you have a provider install you do not need to make any modificaitons to your Vagrantfile Use vagrant up with proper provider and vagrant will do the rest vagrant up --provider=vmware_fusion Snapshot used to manage snapshots with guest machine records point-in-time state of guest machine commands include push pop save restore list delete Snapshot push vagrant snapshot push takes a snapshot and pushes it onto the snapshot stack shorthand for vagrant snapshot save where you do not need to specifiy a name Warning: if using push and pop avoid using save and restore which are unsafe to mix Snapshot Pop vagrant snapshot pop will restore pushed state Options --[no-]provision - Force provisioners to run (or prevent them from doing so) --no-delete - prevents deletion of snapshot after restoring --no-start - prevent guest from being started after restore Other commands vagrant snapshot save [vm-name] NAME vagrant snapshot restore [vm-name] NAME vagrant snapshot list vagrant snapshot delete [vm-name] NAME","title":"Vagrant Tutorial"},{"location":"vagrant_tutorial/#vagrant-tutorial","text":"source","title":"Vagrant tutorial"},{"location":"vagrant_tutorial/#install","text":"Install vagrant via binary package always link sudo apt update curl -O https://releases.hashicorp.com/vagrant/2.2.7/vagrant_2.2.7_x86_64.deb sudo apt install ./vagrant_2.2.7_x86_64.deb vagrant --version","title":"Install"},{"location":"vagrant_tutorial/#project-setup","text":"mkdir <root-project-dir> cd <root-project-dir> vagrant init hashicorp/bionic64","title":"Project Setup"},{"location":"vagrant_tutorial/#boxes","text":"vagrant uses base images to quickly clone a virtual machine base images are known as \"boxes\" boxes are added to vagrant with vagrant box add vagrant box add hashicorp/bionic64 this will download box named hashicorp/bionic64 boxes are globally stored for current user each project uses box as initial image to clone from, but nevet modifies actual base","title":"Boxes"},{"location":"vagrant_tutorial/#using-a-box","text":"open VagrantFile and change the contents to the following Vagrant.configure(\"2\") do |config| config.vm.box = \"hashicorp/bionic64\" # optionally specify explicit version of box config.vm.box_version = \"1.1.0\" # optionally specify url to box directly config.vm.box_url = \"https://vagrantcloud.com/hashicorp/bionc64\" end best place to find more boxes is HashiCorp's Vagrant Cloud box catalog","title":"Using a Box"},{"location":"vagrant_tutorial/#up-and-ssh","text":"# boot vagrant environment vagrant up # ssh into vm vagrant ssh # SSH session can be terminated via <C-d> # destroy vm vagrant destroy # remove downloaded box vagrant box remove be careful not to rm -rf / since vagrant shares a directory at /vagrant with directory on host and this can delete all files in shared folder","title":"Up and SSH"},{"location":"vagrant_tutorial/#synced-folders","text":"by default vagrant shares project directory to /vagrant folder","title":"Synced Folders"},{"location":"vagrant_tutorial/#provisioning","text":"vagrant has built in support for automated provisioning vagrant will automatically install software when you vagrant up","title":"Provisioning"},{"location":"vagrant_tutorial/#installing-apache","text":"# bootstap.sh #!/usr/bin/env bash apt-get update apt-get install -y apache2 if ! [ -L /var/www ]; then rm -rf /var/www ln -fs /vagrant /var/www fi Vagrant.configure(\"2\") do |config| config.vm.box = \"hashicorp/bionic64\" config.vm.provision :shell, path: \"bootstrap.sh\" end create machine with vagrant up and vagrant automatically will provision if guest machine already running, run vagrant reload --provision","title":"Installing apache"},{"location":"vagrant_tutorial/#networking","text":"","title":"Networking"},{"location":"vagrant_tutorial/#port-forwarding","text":"Port forwarding allows to specify ports on guest machine to share via a port on host machine Vagrant.configure(\"2\") do |config| config.vm.box = \"hashicorp/bionic64\" config.vm.provision :shell, path: \"bootstrap.sh\" config.vm.network :forwarded_port, guest: 80, host: 2567 end See more","title":"Port Forwarding"},{"location":"vagrant_tutorial/#share","text":"vagrant shares let you share your environment to anyone who has an internet connection will generate a URL that will route directly to your environment to install run the following command vagrant plugin install vagrant-share next run the following command vagrant share","title":"Share"},{"location":"vagrant_tutorial/#teardown","text":"with vagrant you can suspend , halt , or destroy","title":"Teardown"},{"location":"vagrant_tutorial/#suspending","text":"vagrant suspend will save current running state of machine and stop it. resume work by calling vagrant up it is super fast but eats up disk space and requires more to store state of RAM on disk","title":"Suspending"},{"location":"vagrant_tutorial/#halting","text":"vagrant halt will gracefully shut down guest OS and power down guest machine user vagrant up to boot up again cleanly shuts down machine, preserving contents on disk","title":"Halting"},{"location":"vagrant_tutorial/#destorying","text":"vagrant destroy removes all traces of guest machine","title":"Destorying"},{"location":"vagrant_tutorial/#providers","text":"Vagrant can work with a variety of backend providers such as VMware and Hyper-V Once you have a provider install you do not need to make any modificaitons to your Vagrantfile Use vagrant up with proper provider and vagrant will do the rest vagrant up --provider=vmware_fusion","title":"Providers"},{"location":"vagrant_tutorial/#snapshot","text":"used to manage snapshots with guest machine records point-in-time state of guest machine commands include push pop save restore list delete","title":"Snapshot"},{"location":"vagrant_tutorial/#snapshot-push","text":"vagrant snapshot push takes a snapshot and pushes it onto the snapshot stack shorthand for vagrant snapshot save where you do not need to specifiy a name Warning: if using push and pop avoid using save and restore which are unsafe to mix","title":"Snapshot push"},{"location":"vagrant_tutorial/#snapshot-pop","text":"vagrant snapshot pop will restore pushed state","title":"Snapshot Pop"},{"location":"vagrant_tutorial/#options","text":"--[no-]provision - Force provisioners to run (or prevent them from doing so) --no-delete - prevents deletion of snapshot after restoring --no-start - prevent guest from being started after restore","title":"Options"},{"location":"vagrant_tutorial/#other-commands","text":"vagrant snapshot save [vm-name] NAME vagrant snapshot restore [vm-name] NAME vagrant snapshot list vagrant snapshot delete [vm-name] NAME","title":"Other commands"}]}